<?xml version="1.0" encoding="ISO-8859-1"?>

<bugs>
	<bug>
		<id>2935</id>
		<title>在DynamicTopic并发模式下，偶发线程卡死问题（由template.waitForResult()引起）</title>
		<body>- [ ] I have searched the [issues](https://github.com/alibaba/canal/issues) of this repository and believe that this is not a duplicate. - [ ] I have checked the [FAQ](https://github.com/alibaba/canal/wiki/FAQ) of this repository and believe that this is not a duplicate.  ### environment  * canal version 1.1.5 最新（last modify Mar 13）  ### Issue Description 测试中发现，当设置了dynamicTopic后（topic个数 &gt; 10），偶尔会遇到订阅线程卡死的现象。 此时用jstack查看，发现所有 MQParallel 线程均处于WAITING状态，停留在 MQMessageUtils::buildMessageData()方法的template.waitForResult()代码行，而CanalKafkaProducer的send线程也同样处在template.waitForResult()代码行的WAITING状态。 分析代码可知，CanalKafkaProducer与MQMessageUtils中的并发处理使用的是同一个executor线程池，默认情况下该线程池初始化为8个活动线程，排队长度最大为16。我们可以做以下假设： 假设动态topic解析出8个topic，则会submit 8个并行线程，再假设这8个并行线程都快速运行到了MQMessageUtils::buildMessageData()方法中，于是又会submit更多并行线程用于解析message。由于此时线程池中已有前面的8个topic活动线程占用，新的解析线程请求只能排队等待。然而，线程池中的这8个活动线程此时又都处在了template.waitForResult()位置，等待解析线程先执行完成。 于是，一个死循环等待产生了。线程池中的活动线程等待排队线程先结束，而排队线程又在等待活动线程执行完空出地方。  ### 解决方案 个人建议将两层并行处理分别提交到独立的两个线程池中，如此便不会再有互相等待的问题产生。 经本人于本地测试验证，简单增加一个线程池即可解决以上问题。 </body>
		<created>2020-08-08 07:58:29</created>
		<closed>2020-08-21 09:23:08</closed>
	</bug>
	<bug>
		<id>2755</id>
		<title>使用canal admin管理集群不能自定义instance的spring.xml</title>
		<body>- [x] I have searched the [issues](https://github.com/alibaba/canal/issues) of this repository and believe that this is not a duplicate. - [x] I have checked the [FAQ](https://github.com/alibaba/canal/wiki/FAQ) of this repository and believe that this is not a duplicate.  ### environment  * canal version : 1.1.4.RELEASE    canal admin/server HA  * mysql version  ### Issue Description  使用admin管理集群不能自定义instance的spring.xml  ### Steps to reproduce  以下只写主要相关配置：  1.  canal admin的集群配置文件canal.properties      ```properties     canal.admin.manager = 10.100.12.12:8089     canal.instance.global.mode = manager     canal.instance.global.spring.xml = classpath:spring/default-instance.xml     # 自定义instance的spring.xml     canal.instance.shop-item.spring.xml = classpath:spring/group-instance.xml     # 由于未自动从zk获取写手动填写     canal.destinations = shop-item     ```  2.  导入默认instance模板，修改设置并启动  ### Expected behaviour  以自定义的group-instance.xml加载正常启动运行  ### Actual behaviour  仍然使用global配置的default-instance.xml加载启动，由于instance以group的形式配置，无法获取到mysql地址报错  ### Debug  从代码中发现：  1.  配置了canal.admin.manager参数就会强制设置InstanceConfig.setMode(InstanceMode.MANAGER) 2.  当Mode == InstanceMode.MANAGER时不会设置自定义的example.spring.xml  ```java // com.alibaba.otter.canal.deployer.CanalController          private void initInstanceConfig(Properties properties) {         // 实例名称从admin配置的canal.properties中获取         String destinationStr = getProperty(properties, CanalConstants.CANAL_DESTINATIONS);         String[] destinations = StringUtils.split(destinationStr, CanalConstants.CANAL_DESTINATION_SPLIT);          for (String destination : destinations) {             InstanceConfig config = parseInstanceConfig(properties, destination);             InstanceConfig oldConfig = instanceConfigs.put(destination, config);              if (oldConfig != null) {                 logger.warn("destination:{} old config:{} has replace by new config:{}", destination, oldConfig, config);             }         }     }      private InstanceConfig parseInstanceConfig(Properties properties, String destination) {         String adminManagerAddress = getProperty(properties, CanalConstants.CANAL_ADMIN_MANAGER);         InstanceConfig config = new InstanceConfig(globalInstanceConfig);         String modeStr = getProperty(properties, CanalConstants.getInstanceModeKey(destination));                 // 由于使用canal admin管理集群，必定会配置该参数，此时MODE会被强制设置为MANAGER         if (StringUtils.isNotEmpty(adminManagerAddress)) {             // 如果指定了manager地址,则强制适用manager             config.setMode(InstanceMode.MANAGER);         } else if (StringUtils.isNotEmpty(modeStr)) {             config.setMode(InstanceMode.valueOf(StringUtils.upperCase(modeStr)));         }          String lazyStr = getProperty(properties, CanalConstants.getInstancLazyKey(destination));         if (!StringUtils.isEmpty(lazyStr)) {             config.setLazy(Boolean.valueOf(lazyStr));         }          if (config.getMode().isManager()) {             String managerAddress = getProperty(properties, CanalConstants.getInstanceManagerAddressKey(destination));             if (StringUtils.isNotEmpty(managerAddress)) {                 if (StringUtils.equals(managerAddress, "${canal.admin.manager}")) {                     managerAddress = adminManagerAddress;                 }                 config.setManagerAddress(managerAddress);             }                          // 由于已强制被设置为MANAGER, 不会进入该段逻辑，即不能自定义spring.xml，只能一个集群都使用global.spring.xml         } else if (config.getMode().isSpring()) {             String springXml = getProperty(properties, CanalConstants.getInstancSpringXmlKey(destination));             if (StringUtils.isNotEmpty(springXml)) {                 config.setSpringXml(springXml);             }         }          return config;     } ```    ### Log   If there is an exception, please attach the exception trace:  ```log 2020-05-21 09:58:45.341 [pool-50-thread-1] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties] 2020-05-21 09:58:45.341 [pool-50-thread-1] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [shop-item/instance.properties] 2020-05-21 09:58:45.341 [pool-50-thread-1] WARN  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Could not load properties from class path resource [shop-item/instance.properties]: class path resource [shop-item/instance.properties] cannot be opened because it does not exist 2020-05-21 09:58:45.345 [pool-50-thread-1] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-shop-item  2020-05-21 09:58:45.349 [pool-50-thread-1] WARN  c.a.o.canal.parse.inbound.mysql.dbsync.LogEventConvert - --&gt; init table filter : ^.*\..*$ 2020-05-21 09:58:45.349 [pool-50-thread-1] WARN  c.a.o.canal.parse.inbound.mysql.dbsync.LogEventConvert - --&gt; init table black filter :  2020-05-21 09:58:45.349 [destination = shop-item , address = null , EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - parse events has an error com.alibaba.otter.canal.parse.exception.CanalParseException: illegal connection is null ```</body>
		<created>2020-05-21 07:03:38</created>
		<closed>2020-08-21 03:22:20</closed>
	</bug>
	<bug>
		<id>2735</id>
		<title>CanalRabbitMQConsumer  disconnect() 方法报错</title>
		<body>- [ ] I have searched the [issues](https://github.com/alibaba/canal/issues) of this repository and believe that this is not a duplicate. - [ ] I have checked the [FAQ](https://github.com/alibaba/canal/wiki/FAQ) of this repository and believe that this is not a duplicate.  ### environment  * canal version 1.1.5-alpha-1 * mysql version 5,7  ### Issue Description  CanalRabbitMQConsumer  disconnect() 方法内 channel.close(); 和 connect.close(); 执行先后顺序要互换。不然会导致 启动 或者 关闭服务的时候报错。  ### 修复如下： ```     public void disconnect() {          if (channel != null) {     // 要先关闭 channel             try {                 channel.close();             } catch (IOException | TimeoutException e) {                 throw new CanalClientException("stop channel error", e);             }         }          if (connect != null) {             try {                 connect.close();             } catch (IOException e) {                 throw new CanalClientException("stop connect error", e);             }         }     } ```  ### Steps to reproduce  ### Expected behaviour  ### Actual behaviour   If there is an exception, please attach the exception trace:  ``` 2020-05-12 21:29:58.779 [Thread-2] ERROR c.z.a.datastash.launcher.client.loader.AdapterProcessor - process error! com.rabbitmq.client.AlreadyClosedException: connection is already closed due to clean connection shutdown; protocol method: #method&lt;connection.close&gt;(reply-code=200, reply-text=OK, class-id=0, method-id=0) at com.rabbitmq.client.impl.AMQChannel.processShutdownSignal(AMQChannel.java:401) ~[amqp-client-5.9.0.jar:5.9.0] at com.rabbitmq.client.impl.ChannelN.startProcessShutdownSignal(ChannelN.java:287) ~[amqp-client-5.9.0.jar:5.9.0] at com.rabbitmq.client.impl.ChannelN.close(ChannelN.java:608) ~[amqp-client-5.9.0.jar:5.9.0] at com.rabbitmq.client.impl.ChannelN.close(ChannelN.java:542) ~[amqp-client-5.9.0.jar:5.9.0] at com.rabbitmq.client.impl.ChannelN.close(ChannelN.java:535) ~[amqp-client-5.9.0.jar:5.9.0] at com.rabbitmq.client.impl.recovery.AutorecoveringChannel.close(AutorecoveringChannel.java:73) ~[amqp-client-5.9.0.jar:5.9.0] at com.zmops.argus.datastash.connector.consumer.CanalRabbitMQConsumer.disconnect(CanalRabbitMQConsumer.java:194) ~[argus-datastash-0.0.1-SNAPSHOT.jar:na] at com.zmops.argus.datastash.launcher.client.loader.AdapterProcessor.process(AdapterProcessor.java:221) ~[argus-datastash-0.0.1-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_242] ```</body>
		<created>2020-05-12 15:07:04</created>
		<closed>2020-08-21 03:33:14</closed>
	</bug>
	<bug>
		<id>2714</id>
		<title>中文表名在instance中设置UTF-8编码，显示fetch failed by table meta乱码</title>
		<body>- [√ ] I have searched the [issues](https://github.com/alibaba/canal/issues) of this repository and believe that this is not a duplicate. - [ √] I have checked the [FAQ](https://github.com/alibaba/canal/wiki/FAQ) of this repository and believe that this is not a duplicate.  ### environment  * canal 1.1.4 * mysql 5.7.17  ### Issue Description 修改数据的表名为：canaltest.测试 修改数据后报错 example/example.log:  2020-04-30 15:39:09.687 [destination = example , address = localhost/127.0.0.1:3306 , EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address localhost/127.0.0.1:3306 has an error, retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`canaltest`.`æµè¯` Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`canaltest`.`æµè¯` Caused by: com.google.common.util.concurrent.UncheckedExecutionException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`canaltest`.`æµè¯` at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:195) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:950) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEventForTableMeta(LogEventConvert.java:479) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:280) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:246) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) ~[disruptor-3.4.2.jar:na] at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) ~[disruptor-3.4.2.jar:na] at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.8.0_152] at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.8.0_152] at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[na:1.8.0_152] at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[na:1.8.0_152] at java.lang.Thread.run(Unknown Source) [na:1.8.0_152] Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`canaltest`.`æµè¯` Caused by: java.io.IOException: ErrorPacket [errorNumber=1146, fieldCount=-1, message=Table 'canaltest.æµè¯' doesn't exist, sqlState=42S02, sqlStateMarker=#]  with command: desc `canaltest`.`æµè¯` at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:61) ~[canal.parse.driver-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:106) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:92) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:32) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:63) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:53) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:195) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:950) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEventForTableMeta(LogEventConvert.java:479) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:280) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:246) ~[canal.parse-1.1.5-SNAPSHOT.jar:na] at com.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) ~[disruptor-3.4.2.jar:na] at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) ~[disruptor-3.4.2.jar:na] at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.8.0_152] at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.8.0_152] at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[na:1.8.0_152] at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[na:1.8.0_152] at java.lang.Thread.run(Unknown Source) [na:1.8.0_152] </body>
		<created>2020-04-30 07:44:13</created>
		<closed>2020-08-21 06:45:03</closed>
	</bug>
	<bug>
		<id>2616</id>
		<title>zookeeper中没有cursor或很久才更新一次,必须配置canal.destinations参数才会按照canal.zookeeper.flush.period频率更新cursor值</title>
		<body>## Question canal 1.1.4 mysql 5.7.26 zookeeper 3.4.14 kafka 2.12-2.4.0  单点. 发现zookeeper没有cursor, 没存位点信息啊 ``` [zk: localhost:2181(CONNECTED) 10] ls /otter/canal  [cluster, destinations] [zk: localhost:2181(CONNECTED) 11] ls /otter/canal/cluster [172.16.23.8:11111] [zk: localhost:2181(CONNECTED) 12] ls /otter/canal/destinations [fanboshi] [zk: localhost:2181(CONNECTED) 13] ls /otter/canal/destinations/fanboshi [running, cluster, 1001] [zk: localhost:2181(CONNECTED) 14] ls /otter/canal/destinations/fanboshi/running [] [zk: localhost:2181(CONNECTED) 15] get /otter/canal/destinations/fanboshi/running {"active":true,"address":"172.16.2xx.8:11111"} cZxid = 0x10000053b ctime = Mon Mar 09 18:30:00 CST 2020 mZxid = 0x10000053b mtime = Mon Mar 09 18:30:00 CST 2020 pZxid = 0x10000053b cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x100719607a90008 dataLength = 45 numChildren = 0 [zk: localhost:2181(CONNECTED) 16] ls /otter/canal/destinations/fanboshi/cluster  [172.16.23.8:11111] [zk: localhost:2181(CONNECTED) 17] ls /otter/canal/destinations/fanboshi/1001    [] ```   canal.properties ``` ################################################# #########               common argument         ############# ################################################# # tcp bind ip canal.ip =  # register ip to zookeeper canal.register.ip =  canal.port = 11111 canal.metrics.pull.port = 11112 # canal instance user/passwd # canal.user = canal # canal.passwd = E3619321C1A937C46A0D8BD1DAC39F93B27D4458  # canal admin config #canal.admin.manager = 127.0.0.1:8089 canal.admin.port = 11110 canal.admin.user = admin canal.admin.passwd = 4ACFE3202A5FF5CF467898FC58AAB1D615029441  canal.zkServers = ip1:2181,ip2:2181,ip3:2181 # flush data to zk canal.zookeeper.flush.period = 1000 canal.withoutNetty = false # tcp, kafka, RocketMQ canal.serverMode = kafka # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 ## memory store RingBuffer size, should be Math.pow(2,n) canal.instance.memory.buffer.size = 16384 ## memory store RingBuffer used memory unit size , default 1kb canal.instance.memory.buffer.memunit = 1024  ## meory store gets mode used MEMSIZE or ITEMSIZE canal.instance.memory.batch.mode = MEMSIZE canal.instance.memory.rawEntry = true  ## detecing config canal.instance.detecting.enable = false #canal.instance.detecting.sql = insert into retl.xdual values(1,now()) on duplicate key update x=now() canal.instance.detecting.sql = select 1 canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = false  # support maximum transaction size, more than the size of the transaction will be cut into multiple transactions delivery canal.instance.transaction.size =  1024 # mysql fallback connected to new master should fallback times canal.instance.fallbackIntervalInSeconds = 60  # network config canal.instance.network.receiveBufferSize = 16384 canal.instance.network.sendBufferSize = 16384 canal.instance.network.soTimeout = 30  # binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = false canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = false canal.instance.filter.table.error = false canal.instance.filter.rows = false canal.instance.filter.transaction.entry = false  # binlog format/image check canal.instance.binlog.format = ROW,STATEMENT,MIXED  canal.instance.binlog.image = FULL,MINIMAL,NOBLOB  # binlog ddl isolation canal.instance.get.ddl.isolation = false  # parallel parser config canal.instance.parser.parallel = true ## concurrent thread number, default 60% available processors, suggest not to exceed Runtime.getRuntime().availableProcessors() #canal.instance.parser.parallelThreadSize = 16 ## disruptor ringbuffer size, must be power of 2 canal.instance.parser.parallelBufferSize = 256  # table meta tsdb info canal.instance.tsdb.enable = false canal.instance.tsdb.dir = ${canal.file.data.dir:../conf}/${canal.instance.destination:} canal.instance.tsdb.url = jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername = canal canal.instance.tsdb.dbPassword = canal # dump snapshot interval, default 24 hour canal.instance.tsdb.snapshot.interval = 24 # purge snapshot expire , default 360 hour(15 days) canal.instance.tsdb.snapshot.expire = 360  # aliyun ak/sk , support rds/mq canal.aliyun.accessKey = canal.aliyun.secretKey =  ################################################# #########               destinations            ############# ################################################# canal.destinations =  # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5  canal.instance.tsdb.spring.xml = classpath:spring/tsdb/h2-tsdb.xml #canal.instance.tsdb.spring.xml = classpath:spring/tsdb/mysql-tsdb.xml  canal.instance.global.mode = spring canal.instance.global.lazy = false canal.instance.global.manager.address = ${canal.admin.manager} #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml #canal.instance.global.spring.xml = classpath:spring/file-instance.xml canal.instance.global.spring.xml = classpath:spring/default-instance.xml  ################################################## #########                    MQ                      ############# ################################################## canal.mq.servers = ip1:9092,ip2:9092,ip3:9092 canal.mq.retries = 0 canal.mq.batchSize = 16384 canal.mq.maxRequestSize = 1048576 canal.mq.lingerMs = 100 canal.mq.bufferMemory = 33554432 canal.mq.canalBatchSize = 50 canal.mq.canalGetTimeout = 100 canal.mq.flatMessage = true canal.mq.compressionType = none canal.mq.acks = all #canal.mq.properties. = canal.mq.producerGroup = test # Set this value to "cloud", if you want open message trace feature in aliyun. canal.mq.accessChannel = local # aliyun mq namespace #canal.mq.namespace =  ################################################## #########     Kafka Kerberos Info    ############# ################################################## canal.mq.kafka.kerberos.enable = false canal.mq.kafka.kerberos.krb5FilePath = "../conf/kerberos/krb5.conf" canal.mq.kafka.kerberos.jaasFilePath = "../conf/kerberos/jaas.conf" ```  instance.properties ``` $cat fanboshi/instance.properties  ################################################# ## mysql serverId , v1.0.26+ will autoGen # canal.instance.mysql.slaveId=0  # enable gtid use true/false canal.instance.gtidon=true  # position info canal.instance.master.address=ip:port canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp= canal.instance.master.gtid=c30c6a02-4e32-11ea-84ec-fa163edcd14e:1-2051921  # rds oss binlog canal.instance.rds.accesskey= canal.instance.rds.secretkey= canal.instance.rds.instanceId=  # table meta tsdb info canal.instance.tsdb.enable=false canal.instance.tsdb.url=jdbc:mysql://ip:port/canal_tsdb_fanboshi canal.instance.tsdb.dbUsername=canal canal.instance.tsdb.dbPassword=canal  #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = #canal.instance.standby.gtid=  # username/password canal.instance.dbUsername=canal_r canal.instance.dbPassword=superpassword canal.instance.connectionCharset = UTF-8 # enable druid Decrypt database password canal.instance.enableDruid=false #canal.instance.pwdPublicKey=MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBALK4BUxdDltRRE5/zXpVEVPUgunvscYFtEip3pmLlhrWpacX7y7GCMo2/JM6LeHmiiNdH1FWgGCpUfircSwlWKUCAwEAAQ==  # table regex canal.instance.filter.regex=fanboshi\\..*,sysbench\\..* # table black regex canal.instance.filter.black.regex=.*\\.\\_.*\\_ghc,.*\\.\\_.*\\_gho,.*\\.\\_.*\\_del # table field filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2) #canal.instance.filter.field=test1.t_product:id/subject/keywords,test2.t_company:id/name/contact/ch # table field black filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2) #canal.instance.filter.black.field=test1.t_product:subject/product_image,test2.t_company:id/name/contact/ch  # mq config canal.mq.topic=default_topic # dynamic topic route by schema or table regex #canal.mq.dynamicTopic=mytest1.user,mytest2\\..*,.*\\..* canal.mq.dynamicTopic=.*\\..* #canal.mq.partition=0 # hash partition config #canal.mq.partitionsNum=3 #canal.mq.partitionHash=test.table:id^name,.*\\..* ################################################# ```</body>
		<created>2020-03-09 10:37:30</created>
		<closed>2020-08-21 07:57:08</closed>
	</bug>
	<bug>
		<id>2602</id>
		<title>lazy set in PrometheusCanalEventDownStreamHandler will cause metric not accurate</title>
		<body>![image](https://user-images.githubusercontent.com/4374015/75602499-e64aa280-5b00-11ea-8be6-f3ad5ba6068b.png) </body>
		<created>2020-02-29 06:36:47</created>
		<closed>2020-08-21 08:08:42</closed>
	</bug>
	<bug>
		<id>2596</id>
		<title>Aliyun RDS local binlog dump bug</title>
		<body>- [x] I have searched the [issues](https://github.com/alibaba/canal/issues) of this repository and believe that this is not a duplicate. - [x] I have checked the [FAQ](https://github.com/alibaba/canal/wiki/FAQ) of this repository and believe that this is not a duplicate.  ### environment  * canal version 1.1.4  * mysql version all  ### Issue Description when finish binlog dump mode ,back to normal, dumpErrorCount never be clean , then go into binlog dump mode again, but position is not belong binlog file in OSS , endless retrying ...  ### Steps to reproduce set history binlog  position in meta.log(or other style). start canal. wait for local binlog mode exist.  step in the code .  ![bugfix](https://user-images.githubusercontent.com/826821/75432682-c0e75880-5989-11ea-9db5-903806b62b0c.jpg)   ### Expected behaviour when local binlog mode end and go back to normal   ### Actual behaviour endless retrying in local binlog mode  If there is an exception, please attach the exception trace:  none</body>
		<created>2020-02-27 09:49:39</created>
		<closed>2020-02-28 04:33:48</closed>
	</bug>
	<bug>
		<id>2585</id>
		<title>1.1.4 的canal.instance.connectionCharset 不能识别</title>
		<body>- [ ] I have searched the [issues](https://github.com/alibaba/canal/issues) of this repository and believe that this is not a duplicate. - [ ] I have checked the [FAQ](https://github.com/alibaba/canal/wiki/FAQ) of this repository and believe that this is not a duplicate.  ### environment  * canal version 1.1.4 * mysql version 5.5.6 ### Issue Description canal开启后日志里面提示： o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)]  ### Steps to reproduce  ### Expected behaviour  ### Actual behaviour   If there is an exception, please attach the exception trace:  ``` Just put your stack trace here! ```</body>
		<created>2020-02-21 03:51:17</created>
		<closed>2020-08-21 08:15:37</closed>
	</bug>
	<bug>
		<id>2502</id>
		<title>CanalRocketMQProducer#sendMessage数组越界异常</title>
		<body>- [ ] I have searched the [issues](https://github.com/alibaba/canal/issues) of this repository and believe that this is not a duplicate. - [ ] I have checked the [FAQ](https://github.com/alibaba/canal/wiki/FAQ) of this repository and believe that this is not a duplicate.  ### environment  * canal version * mysql version  ### Issue Description ```java com.alibaba.otter.canal.rocketmq.CanalRocketMQProducer#sendMessage   public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) {  if (partition &gt; mqs.size()) {  return mqs.get(partition % mqs.size());  } else {  return mqs.get(partition);  }  }  如果 partition == mqs.size() 会产生ArrayIndexOutOfBoundsException  ```  ### Steps to reproduce  ### Expected behaviour  ### Actual behaviour   If there is an exception, please attach the exception trace:  ``` Just put your stack trace here! ```</body>
		<created>2019-12-26 01:33:43</created>
		<closed>2020-08-21 08:34:29</closed>
	</bug>
	<bug>
		<id>2434</id>
		<title>CanalRocketMQProducer 通过ExecutorTemplate并发执行，线程池出现一直等待现象</title>
		<body>- [ ] I have searched the [issues](https://github.com/alibaba/canal/issues) of this repository and believe that this is not a duplicate. - [ ] I have checked the [FAQ](https://github.com/alibaba/canal/wiki/FAQ) of this repository and believe that this is not a duplicate.  ### environment  * canal version      v1.1.5-alpha-1 * mysql version    5.7 ### Issue Description ``` CanalRocketMQProducer 通过ExecutorTemplate并发执行时，由于提交到线程池里的分表任务、buildMessageData任务、分区发送任务有先后依赖，会出现worker线程相互依赖导致woker线程及主线程都分表任务占用，又一直等待buildMessageData任务、分区发送任务执行，出现一直等待现象 ```  ### Steps to reproduce  ### Expected behaviour 线程池里的worker任务不应该产生依赖 ### Actual behaviour   If there is an exception, please attach the exception trace:  ``` Just put your stack trace here! ```</body>
		<created>2019-11-26 15:00:47</created>
		<closed>2020-08-21 09:22:01</closed>
	</bug>
	<bug>
		<id>2418</id>
		<title>关于FileMixedMetaManager#stop方法的实现</title>
		<body>- 调用父类的stop()方法时，其实已经清除了destinations中的数据，其后调用 flushDataToFile()方法刷新数据已经不起作用了 ```java public void stop() {         super.stop();          flushDataToFile();// 刷新数据         executor.shutdownNow();         destinations.clear();         batches.clear();     } ``` </body>
		<created>2019-11-21 07:12:06</created>
		<closed>2020-08-22 00:57:07</closed>
	</bug>
	<bug>
		<id>2399</id>
		<title>gtid模式下ack不了消息</title>
		<body>版本：1.1.5-SNAPSHOT  关键配置如下： ```shell canal.serverMode = kafka   canal.instance.gtidon=true ```  问题描述： `filterTransactionEntry`属性在MQProperties类中默认为`true`，此为问题根源 ![image](https://user-images.githubusercontent.com/1318274/68846993-d8ffd880-0708-11ea-9632-6ba206427a27.png) `EntryEventSink`在put消息时会有以下动作（99行左右）： ```java // filterTransactionEntry 为true       if (filterTransactionEntry                 &amp;&amp; (entry.getEntryType() == EntryType.TRANSACTIONBEGIN || entry.getEntryType() == EntryType.TRANSACTIONEND)) {                 long currentTimestamp = entry.getHeader().getExecuteTime();                 // 基于一定的策略控制，放过空的事务头和尾，便于及时更新数据库位点，表明工作正常                 if (lastTransactionCount.incrementAndGet() &lt;= emptyTransctionThresold                     &amp;&amp; Math.abs(currentTimestamp - lastTransactionTimestamp) &lt;= emptyTransactionInterval) {  //TRANSACTIONEND类型下currentTimestamp=lastTransactionTimestamp，整个表达式会满足导致跳过event的添加                     continue;                 } else {                     lastTransactionCount.set(0L);                     lastTransactionTimestamp = currentTimestamp;                 }             }       ....        ...       events.add(event); ``` `MemoryEventStoreWithBuffer`在get消息时会设置ack（340行左右）： ```java         for (int i = entrys.size() - 1; i &gt;= 0; i--) {             Event event = entrys.get(i);             // GTID模式,ack的位点必须是事务结尾,因为下一次订阅的时候mysql会发送这个gtid之后的next,如果在事务头就记录了会丢这最后一个事务             if ((CanalEntry.EntryType.TRANSACTIONBEGIN == event.getEntryType() &amp;&amp; StringUtils.isEmpty(event.getGtid()))                 || CanalEntry.EntryType.TRANSACTIONEND == event.getEntryType() || isDdl(event.getEventType())) {  //永远不可能满足分支，entrys不可能会有TRANSACTIONEND的消息                 // 将事务头/尾设置可被为ack的点                 range.setAck(CanalEventUtils.createPosition(event));                 break;             }         } ``` 最终`CanalServerWithEmbedded`无法进行ack操作（451行左右）： ```         // 更新cursor         if (positionRanges.getAck() != null) { //永远为null             canalInstance.getMetaManager().updateCursor(clientIdentity, positionRanges.getAck());             if (logger.isInfoEnabled()) {                 logger.info("ack successfully, clientId:{} batchId:{} position:{}",                     clientIdentity.getClientId(),                     batchId,                     positionRanges);             }         } ```</body>
		<created>2019-11-14 10:12:38</created>
		<closed>2020-08-22 01:04:44</closed>
	</bug>
	<bug>
		<id>2369</id>
		<title>中文表名时解析异常</title>
		<body>Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`kiddo`.`ç¬&lt;94&gt;è®°ç»&lt;9f&gt;è®¡` Caused by: java.io.IOException: ErrorPacket [errorNumber=1146, fieldCount=-1, message=Table 'kiddo.ç¬&lt;94&gt;è®°ç»&lt;9f&gt;è®¡' doesn't exist, sqlState=42S02, sqlStateMarker=#]  with command: show create table `kiddo`.`ç¬&lt;94&gt;è®°ç»&lt;9f&gt;è®¡`         at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:61)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:106)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:177)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:950)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEventForTableMeta(LogEventConvert.java:479)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:274)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:246)         at com.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168)         at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125)         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)         at java.util.concurrent.FutureTask.run(FutureTask.java:266)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)         at java.lang.Thread.run(Thread.java:748) </body>
		<created>2019-11-07 02:04:26</created>
		<closed>2020-08-22 01:25:45</closed>
	</bug>
	<bug>
		<id>2257</id>
		<title>MySQL time类型字段，值为100:00:01时解析的值错误，解析为00:00:01</title>
		<body>- [x] I have searched the [issues](https://github.com/alibaba/canal/issues) of this repository and believe that this is not a duplicate. - [x] I have checked the [FAQ](https://github.com/alibaba/canal/wiki/FAQ) of this repository and believe that this is not a duplicate.  ### environment  * canal version  1.1.3 * mysql version  5.7  ### Issue Description MySQL字段类型为TIME时，写入值100:00:01，canal解析出来的值为00:00:01。大于100的时间，例如101:00:01，解析正常。  debug研究了下，应该是RowsLogBuffer解析的bug，第759行，个人觉得应该是d &gt;= 100。 ```            if (d &gt; 100) {                         builder.append(String.valueOf(d));                     } else {                         appendNumber2(builder, d);                     } ``` ```   public static void appendNumber2(StringBuilder builder, int d) {         if (d &gt;= 10) {             builder.append(digits[(d / 10) % 10]).append(digits[d % 10]);         } else {             builder.append('0').append(digits[d]);         }     } ``` appendNumber2方法里会丢失精度，如果是100，格式化完返回00。   </body>
		<created>2019-09-26 09:44:28</created>
		<closed>2019-10-09 05:39:56</closed>
	</bug>
	<bug>
		<id>2197</id>
		<title>server端启动报错：LogbackException</title>
		<body>## Question 操作时跟着官方demo一步一步来的， [https://github.com/alibaba/canal/wiki/QuickStart](https://github.com/alibaba/canal/wiki/QuickStart)  结果  ch.qos.logback.core.LogbackException:   详细日志信息如下：  `start cmd :  java   -Xms128m -Xmx512m -XX:PermSize=128m  -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Dapplication.codeset=UTF-8 -Dfile.encoding=UTF-8  -server -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,address=9099,server=y,suspend=n  -DappName=otter-canal -Dlogback.configurationFile="" -Dcanal.conf="D:\Codes\canal-server\bin\\..\conf\canal.properties" -classpath "D:\Codes\canal-server\bin\\..\conf\..\lib\*;D:\Codes\canal-server\bin\\..\conf" java   -Xms128m -Xmx512m -XX:PermSize=128m  -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Dapplication.codeset=UTF-8 -Dfile.encoding=UTF-8  -server -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,address=9099,server=y,suspend=n  -DappName=otter-canal -Dlogback.configurationFile="" -Dcanal.conf="D:\Codes\canal-server\bin\\..\conf\canal.properties" -classpath "D:\Codes\canal-server\bin\\..\conf\..\lib\*;D:\Codes\canal-server\bin\\..\conf" com.alibaba.otter.canal.deployer.CanalLauncher Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0 Listening for transport dt_socket at address: 9099 Failed to instantiate [ch.qos.logback.classic.LoggerContext] Reported exception: ch.qos.logback.core.LogbackException: Unexpected filename extension of file [file:/D:/Codes/canal-server/conf/]. Should be either .groovy or .xml         at ch.qos.logback.classic.util.ContextInitializer.configureByResource(ContextInitializer.java:79)         at ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:152)         at org.slf4j.impl.StaticLoggerBinder.init(StaticLoggerBinder.java:85)         at org.slf4j.impl.StaticLoggerBinder.&lt;clinit&gt;(StaticLoggerBinder.java:55)         at org.slf4j.LoggerFactory.bind(LoggerFactory.java:141)         at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:120)         at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:331)         at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:283)         at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:304)         at com.alibaba.otter.canal.deployer.CanalLauncher.&lt;clinit&gt;(CanalLauncher.java:29)` </body>
		<created>2019-09-16 01:34:05</created>
		<closed>2019-09-16 01:58:57</closed>
	</bug>
	<bug>
		<id>2168</id>
		<title>canal 创建数据库报错： CREATE DATABASE `crm_sales` CHARACTER SET 'utf8' COLLATE 'utf8_general_ci' com.alibaba.fastsql.sql.parser.ParserException: syntax error, error in :'CHARACTER SET 'utf8' COLLATE 'utf8_gener', expect null, actual null, pos 48, line 1, column 43, token LITERAL_CHARS utf8</title>
		<body>2019-09-09 09:19:45.240 [MultiStageCoprocessor-other-example-0] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE DATABASE `crm_sales` CHARACTER SET 'utf8' COLLATE 'utf8_general_ci' com.alibaba.fastsql.sql.parser.ParserException: syntax error, error in :'CHARACTER SET 'utf8' COLLATE 'utf8_gener', expect null, actual null, pos 48, line 1, column 43, token LITERAL_CHARS utf8 at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:430) ~[fastsql-2.0.0_preview_973.jar:2.0.0_preview_973] at com.alibaba.fastsql.sql.parser.SQLParser.accept(SQLParser.java:438) ~[fastsql-2.0.0_preview_973.jar:2.0.0_preview_973] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseCreateDatabase(MySqlStatementParser.java:7756) ~[fastsql-2.0.0_preview_973.jar:2.0.0_preview_973] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:307) ~[fastsql-2.0.0_preview_973.jar:2.0.0_preview_973] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:231) ~[fastsql-2.0.0_preview_973.jar:2.0.0_preview_973] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:536) ~[fastsql-2.0.0_preview_973.jar:2.0.0_preview_973] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:439) ~[fastsql-2.0.0_preview_973.jar:2.0.0_preview_973] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:83) ~[canal.parse-1.1.4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:156) [canal.parse-1.1.4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:238) [canal.parse-1.1.4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:273) [canal.parse-1.1.4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:118) [canal.parse-1.1.4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:292) [canal.parse-1.1.4.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:246) [canal.parse-1.1.4.jar:na] at com.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) [disruptor-3.4.2.jar:na] at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) [disruptor-3.4.2.jar:na] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_211] at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_211] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_211] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_211] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_211] 2019-09-09 09:19:45.245 [MultiStageCoprocessor-other-example-0] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : CREATE DATABASE `crm_sales` CHARACTER SET 'utf8' COLLATE 'utf8_general_ci' </body>
		<created>2019-09-09 01:28:59</created>
		<closed>2019-09-12 01:10:06</closed>
	</bug>
	<bug>
		<id>2127</id>
		<title>canal日志写满磁盘</title>
		<body>- [ ] I have searched the [issues](https://github.com/alibaba/canal/issues) of this repository and believe that this is not a duplicate. - [ ] I have checked the [FAQ](https://github.com/alibaba/canal/wiki/FAQ) of this repository and believe that this is not a duplicate.  ### environment  * canal version 1.1.3 * mysql version 5.7  ### Issue Description   ### Steps to reproduce  ### Expected behaviour  ### Actual behaviour   If there is an exception, please attach the exception trace:  ``` Just put your stack trace here! ```  ![image](https://user-images.githubusercontent.com/9107702/64089357-48a42f80-cd78-11e9-8950-1647fdc79d70.png) </body>
		<created>2019-09-02 03:53:38</created>
		<closed>2019-09-02 06:12:13</closed>
	</bug>
	<bug>
		<id>1826</id>
		<title>canal集成kafka，配置多个destinations，造成kafka事务异常</title>
		<body>- [x] I have searched the [issues](https://github.com/alibaba/canal/issues) of this repository and believe that this is not a duplicate. - [x] I have checked the [FAQ](https://github.com/alibaba/canal/wiki/FAQ) of this repository and believe that this is not a duplicate.  ### environment  * canal version 1.1.3 * mysql version 5.7.22 * kafka version 1.1.1  ### Issue Description canal集成kafka，配置多个destinations，造成kafka事务异常。 每一个destination起一个线程订阅canal，当线程1执行完beginTransaction()时，线程2执行beginTransaction()就会造成kafka事务异常，如下：  exception trace: ``` ERROR com.alibaba.otter.canal.server.CanalMQStarter - TransactionalId canal-transactional-id: Invalid transition attempted from state IN_TRANSACT ION to state IN_TRANSACTION org.apache.kafka.common.KafkaException: TransactionalId canal-transactional-id: Invalid transition attempted from state IN_TRANSACTION to state IN_TRANSACTION         at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:758) ~[kafka-clients-1.1.1.jar:na]         at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:751) ~[kafka-clients-1.1.1.jar:na]         at org.apache.kafka.clients.producer.internals.TransactionManager.beginTransaction(TransactionManager.java:216) ~[kafka-clients-1.1.1.jar:na]         at org.apache.kafka.clients.producer.KafkaProducer.beginTransaction(KafkaProducer.java:587) ~[kafka-clients-1.1.1.jar:na]         at com.alibaba.otter.canal.kafka.CanalKafkaProducer.send(CanalKafkaProducer.java:106) ~[canal.server-1.1.3.jar:na]         at com.alibaba.otter.canal.server.CanalMQStarter.worker(CanalMQStarter.java:182) [canal.server-1.1.3.jar:na]         at com.alibaba.otter.canal.server.CanalMQStarter.access$500(CanalMQStarter.java:22) [canal.server-1.1.3.jar:na]         at com.alibaba.otter.canal.server.CanalMQStarter$CanalMQRunnable.run(CanalMQStarter.java:224) [canal.server-1.1.3.jar:na]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_171]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_171]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] ```</body>
		<created>2019-05-21 03:56:23</created>
		<closed>2019-06-26 08:03:04</closed>
	</bug>
	<bug>
		<id>1762</id>
		<title>JSON类型"null"值bug</title>
		<body>- [x] I have searched the [issues](https://github.com/alibaba/canal/issues) of this repository and believe that this is not a duplicate. - [x] I have checked the [FAQ](https://github.com/alibaba/canal/wiki/FAQ) of this repository and believe that this is not a duplicate.  ### environment  * canal version 1.1.3 * mysql version 5.7  ### Issue Description canal 把 json 类型字段的 "null" 值转换成了 "NULL" 值，再往 mysql 插入这个字段会报错，json "NULL" 无法解析  [MySQL](https://dev.mysql.com/doc/refman/8.0/en/json.html) 遵循 [RFC 7159](https://tools.ietf.org/html/rfc7159) 规范使用 JSON 规范里面表明 JSON 的几个 literal 类型必须是小写：  &gt; 3.  Values    A JSON value MUST be an object, array, number, or string, or one of    the following three literal names:       false null true    The literal names MUST be lowercase.  No other literal names are    allowed.       value = false / null / true / object / array / number / string  所以 MySQL 的 json 字段合法值要么是字符串 "null"，要么是空值 null/NULL，而字符串 "NULL" 不合法  canal 代码在：https://github.com/alibaba/canal/blob/master/dbsync/src/main/java/com/taobao/tddl/dbsync/binlog/JsonConversion.java ``` case LITERAL_NULL:     buf.append("NULL");     break ``` 不知道这里的考虑是怎样的，能否改回 "null" ？  ### Steps to reproduce create table t (data json); insert into t values ('null'); insert into t values ('NULL'); insert into t values (NULL);   If there is an exception, please attach the exception trace:  ``` mysql&gt; insert into t values ('NULL'); ERROR 3140 (22032): Invalid JSON text: "Invalid value." at position 0 in value for column 't.data'. ```</body>
		<created>2019-04-28 12:54:06</created>
		<closed>2019-08-21 06:33:35</closed>
	</bug>
	<bug>
		<id>1672</id>
		<title>请问1.13 flatMessages为何没有同步发送kafka</title>
		<body>请问1.13源码里，CanalKafkaProducer类在发送probuf数据时使用同步发送方式，发送json时候仍然使用异步发送，之前用异步会出现丢数情况，请问这里是否应该改成同步发送   private void produce(String topicName, int partition, FlatMessage flatMessage) throws ExecutionException,                                                                                   InterruptedException {         ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;String, String&gt;(topicName,             partition,             null,             JSON.toJSONString(flatMessage, SerializerFeature.WriteMapNullValue));         if (kafkaProperties.getTransaction()) {             producer2.send(record);         } else {             producer2.send(record);         }     }</body>
		<created>2019-04-08 06:58:40</created>
		<closed>2019-04-08 11:57:36</closed>
	</bug>
	<bug>
		<id>1660</id>
		<title>mysql8 Client Authentication:ErrorPacket </title>
		<body>### environment  * canal version 1.1.3-snatshop * mysql version 8.0.14   mysql已经执行下列语句: CREATE USER canal IDENTIFIED BY 'canal';   GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'canal'@'%'; GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ; FLUSH PRIVILEGES;  授权了用户canal,mysql5.7可以链接,但是8.0.14报异常,在之前的版本也看到类似问题,但是这里没法应用  ``` 2019-04-03 13:25:59.063 [destination = example , address = /****:3306 , EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /47.107.227.75:3306 has an error, retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /47.107.227.75:3306 failure Caused by: java.io.IOException: connect /****:3306 failure         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:82) ~[canal.parse.driver-1.1.3-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:89) ~[canal.parse-1.1.3-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) ~[canal.parse-1.1.3-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:175) ~[canal.parse-1.1.3-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171] Caused by: java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045, fieldCount=-1, message=Access denied for user 'canal'@'****' (using password: YES), sqlState=28000, sqlStateMarker=#]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:250) ~[canal.parse.driver-1.1.3-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:79) ~[canal.parse.driver-1.1.3-SNAPSHOT.jar:na]         ... 4 common frames omitted 2019-04-03 13:25:59.063 [destination = example , address = /****:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: java.io.IOException: connect /***:3306 failure Caused by: java.io.IOException: connect /****:3306 failure         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:82)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:89)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:175)         at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045, fieldCount=-1, message=Access denied for user 'canal'@'****' (using password: YES), sqlState=28000, sqlStateMarker=#]         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:250)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:79)         ... 4 more  ```</body>
		<created>2019-04-03 05:30:00</created>
		<closed>2019-04-03 15:45:21</closed>
	</bug>
	<bug>
		<id>1636</id>
		<title>v1.1.3-alpha-3版kafka主题为多分区时，消费到的数据不完整</title>
		<body>### environment  * canal version v1.1.3-alpha-3 * mysql version 5.6  ### Issue Description  kafka主题为多分区时，消费端消费到的数据不完整  canal.properties主要配置如下： ######### binlog filter config canal.instance.filter.druid.ddl = true canal.instance.filter.query.dcl = true canal.instance.filter.query.dml = false canal.instance.filter.query.ddl = true canal.instance.filter.table.error = true canal.instance.filter.rows = false canal.instance.filter.transaction.entry = false ################################################## #########      MQ      ############# ################################################## canal.mq.servers = 192.168.1.15:9092,192.169.1.16:9092,192.168.1.15:9092 canal.mq.retries = 3 canal.mq.batchSize = 16384 canal.mq.maxRequestSize = 1048576 canal.mq.lingerMs = 1 canal.mq.bufferMemory = 33554432 canal.mq.canalBatchSize = 50 canal.mq.canalGetTimeout = 100 canal.mq.flatMessage = false canal.mq.compressionType = none canal.mq.acks = all #########  use transaction for kafka flatMessage batch produce canal.mq.transaction = false  instance.properties主要配置如下： ######### table regex canal.instance.filter.regex=schema.table ######### table black regex canal.instance.filter.black.regex= ######### mq config canal.mq.topic=topic canal.mq.partition=0 ######### hash partition config canal.mq.partitionsNum=8 canal.mq.partitionHash=.\*\\\\..\*  ######### kafka消费端 ######## @KafkaListener(topics = "${spring.kafka.consumer.topic}",     containerFactory = "kafkaListenerContainerFactory")     public void consumerListener(KafkaMessage message, Acknowledgment ack) {         try {         boolean success = true;         Message canalMessage = message.getMessage();             if(canalMessage != null) {                     if (canalMessage.getId() != -1 &amp;&amp;                      canalMessage.getEntries().size() &gt; 0) {                     success = printEntry(canalMessage.getEntries());                     }             }             if(success)             ack.acknowledge();         }catch (Exception e) {         logger.error(e.getMessage(), e); }     }  ### Steps to reproduce  ### Expected behaviour  ### Actual behaviour </body>
		<created>2019-03-26 14:18:41</created>
		<closed>2019-03-28 02:18:27</closed>
	</bug>
	<bug>
		<id>1626</id>
		<title>表字段位置改变，数据同步错乱</title>
		<body>### environment  * canal version  v1.1.3 * mysql version v5.7.20 * kafka version v1.0  ### Issue Description  &gt; 在同步mysql数据时，如果移动表字段位置，canal只捕获alter操作但没有处理，导致数据同步错乱。在canal  --&gt;  kafka、adapter的hbase  均重现该问题   ### Steps to reproduce  比如重现步骤： ``` 1. 创建了一个hello库quniya4表 2. 表里有三个字段（name,value,id） 3. 我先建的name，value，最后建的id主键自增。现在我把id的位置移动到name、value的上面，    也就是说id的字段位置变成了 frist，现在的表字段位置为（id,name,value） 4. 我插入数据（1,a,aaa）=&gt; (aaa,1,a) ; name变成了1，value变成了a，    而id还是没有移动位置前的第三位，所以kafka消费这条数据时id=aaa，hbase亦是如此 ```    ### Expected behaviour  kafka预期显示： ``` {"data":[{"id":"1","name":"a","value":"aaa"}]      ...} ```  ### Actual behaviour  kafka结果显示： ``` {"data":[{"name":"1","value":"a","id":"aaa"}],"database":"hello","es":1553330817000,"id":20,"isDdl":false,"mysqlType":{"name":"varchar(255)","value":"varchar(255)","id":"int"},"old":null,"pkNames":["id"],"sql":"","sqlType":{"name":4,"value":12,"id":12},"table":"quniya4","ts":1553330817903,"type":"INSERT"} ```  hbase结果显示： ``` hbase(main):009:0&gt; scan 'hello.quniya4' ROW                 COLUMN+CELL                                            ccc|3              column=cf:id, timestamp=1553331439400, value=ccc       ccc|3              column=cf:name, timestamp=1553331439400, value=3       ccc|3              column=cf:value, timestamp=1553331439400, value=c ```        </body>
		<created>2019-03-23 09:33:30</created>
		<closed>2019-04-03 02:59:17</closed>
	</bug>
	<bug>
		<id>1546</id>
		<title>canal无法检测到删除数据库的操作</title>
		<body>### canal-1.1.3无法检测到删除数据库的操作  * canal version : canal-1.1.3 * mysql version :  5.6.15-log MySQL Community Server  ###  canal-1.1.3可以检测到create database操作，但检测不到drop database ‘dbname’操作； 固定topic和动态创建topic都检测不到该操作  ### Steps to reproduce  ### Expected behaviour  ### Actual behaviour </body>
		<created>2019-02-27 04:22:21</created>
		<closed>2019-02-28 02:48:19</closed>
	</bug>
	<bug>
		<id>1532</id>
		<title>MySQL binlog 表名大小写 产生的kafka动态匹配规则错误</title>
		<body>### environment  * canal version   1.1.3 * mysql version  5.7  ### Issue Description 在集成kafka的情况下，canal在拉取到binlog之后，匹配topic的时候，如果topic字母相同但大小写不同会引发kafka的日志恢复并resize，报错如下：（之前已有一个topic：redis_cachecloud_QRTZ_FIRED_TRIGGERS） ERROR Error while creating log for redis_cachecloud_qrtz_fired_triggers-0 in dir E:\kafka\logs\kafka2 (kafka.server.LogDirFailureChannel) java.io.IOException: 请求的操作无法在使用用户映射区域打开的文件上执行 at java.io.RandomAccessFile.setLength(Native Method) at kafka.log.AbstractIndex$$anonfun$resize$1.apply$mcZ$sp(AbstractIndex.scala:186) at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:173) at kafka.log.AbstractIndex$$anonfun$resize$1.apply(AbstractIndex.scala:173) at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251) at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173) at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply$mcZ$sp(AbstractIndex.scala:242) at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:242) at kafka.log.AbstractIndex$$anonfun$trimToValidSize$1.apply(AbstractIndex.scala:242) at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251) at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:241) at kafka.log.LogSegment.recover(LogSegment.scala:377) at kafka.log.Log.kafka$log$Log$$recoverSegment(Log.scala:467) at kafka.log.Log.kafka$log$Log$$recoverLog(Log.scala:581) at kafka.log.Log$$anonfun$2.apply$mcJ$sp(Log.scala:552) at kafka.log.Log$$anonfun$2.apply(Log.scala:552) at kafka.log.Log$$anonfun$2.apply(Log.scala:552) at kafka.log.Log.retryOnOffsetOverflow(Log.scala:1938) at kafka.log.Log.loadSegments(Log.scala:551) at kafka.log.Log.&lt;init&gt;(Log.scala:276) at kafka.log.Log$.apply(Log.scala:2071) at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:691) at kafka.log.LogManager$$anonfun$getOrCreateLog$1.apply(LogManager.scala:659) at scala.Option.getOrElse(Option.scala:121) at kafka.log.LogManager.getOrCreateLog(LogManager.scala:659) at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:199) at kafka.cluster.Partition$$anonfun$getOrCreateReplica$1.apply(Partition.scala:195) at kafka.utils.Pool$$anon$2.apply(Pool.scala:61) at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660) at kafka.utils.Pool.getAndMaybePut(Pool.scala:60) at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:194) at kafka.cluster.Partition$$anonfun$makeFollower$1$$anonfun$apply$mcZ$sp$3.apply(Partition.scala:439) at kafka.cluster.Partition$$anonfun$makeFollower$1$$anonfun$apply$mcZ$sp$3.apply(Partition.scala:439) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at kafka.cluster.Partition$$anonfun$makeFollower$1.apply$mcZ$sp(Partition.scala:439) at kafka.cluster.Partition$$anonfun$makeFollower$1.apply(Partition.scala:431) at kafka.cluster.Partition$$anonfun$makeFollower$1.apply(Partition.scala:431) at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251) at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:259) at kafka.cluster.Partition.makeFollower(Partition.scala:431) at kafka.server.ReplicaManager$$anonfun$makeFollowers$3.apply(ReplicaManager.scala:1244) at kafka.server.ReplicaManager$$anonfun$makeFollowers$3.apply(ReplicaManager.scala:1238) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130) at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130) at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40) at scala.collection.mutable.HashMap.foreach(HashMap.scala:130) at kafka.server.ReplicaManager.makeFollowers(ReplicaManager.scala:1238) at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1076) at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:185) at kafka.server.KafkaApis.handle(KafkaApis.scala:110) at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69) at java.lang.Thread.run(Thread.java:748)  ### Steps to reproduce 既然数据库不区分大小写，那为何动态匹配规则的时候要弄成两个topic？可否修改？  ### Expected behaviour 正确匹配  ### Actual behaviour broker报错</body>
		<created>2019-02-21 07:59:02</created>
		<closed>2019-02-28 02:32:56</closed>
	</bug>
	<bug>
		<id>1374</id>
		<title>如果配置了这个参数，会导致Insert语句的Binlog数据不能同步 canal.mq.partitionHash=</title>
		<body>如果配置了这个参数，会导致Insert语句的Binlog数据不能同步 canal.mq.partitionHash=  补充 ： 我在instance.properties 配置如下，insert语句同步不到kafka canal.instance.filter.regex=bigdata\\.testtable.* canal.mq.topic=example8 canal.mq.partitionsNum=3 canal.mq.partitionHash=bigdata.testtable:uid  后来我修改成下面配置(canal.mq.partitionHash没有设置)，就可以在kafka中获取到insert语句了 canal.instance.filter.regex=bigdata\\.testtable.* canal.mq.topic=example8 canal.mq.partitionsNum=3 canal.mq.partitionHash=   </body>
		<created>2019-01-03 11:01:30</created>
		<closed>2019-01-07 05:40:16</closed>
	</bug>
	<bug>
		<id>1363</id>
		<title>AbstractRequest 类中 一旦设置成utc时区会造成格式化时间错误</title>
		<body>### environment  * canal version * mysql version 5.6  ### Issue Description AbstractRequest 类中 一旦设置成utc时区会造成格式化时间错误  ![image](https://user-images.githubusercontent.com/5573184/50618859-604cbf00-0f30-11e9-89be-d431c9ecef7f.png) ![image](https://user-images.githubusercontent.com/5573184/50618869-75295280-0f30-11e9-9a99-9a426e1a381f.png)  ![image](https://user-images.githubusercontent.com/5573184/50618832-3f846980-0f30-11e9-94f7-161d6f10c8b4.png)   ### Steps to reproduce  ### Expected behaviour  ### Actual behaviour </body>
		<created>2019-01-03 00:22:21</created>
		<closed>2019-01-03 03:48:56</closed>
	</bug>
	<bug>
		<id>1343</id>
		<title>从canal v1.1.1升级到1.1.2 canal.log报错</title>
		<body>### environment  * canal version 1.1.2 * mysql version 8.0.3 * docker version 18.x ### Issue Description 2018-12-26 11:36:30.202 [destination = chess , address = mysql/10.0.0.5:3306 , EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - ---&gt; begin to find start position, it will be long time for reset or first position 2018-12-26 11:36:30.202 [destination = chess , address = mysql/10.0.0.5:3306 , EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just show master status 2018-12-26 11:36:30.204 [destination = chess , address = mysql/10.0.0.5:3306 , EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address mysql/10.0.0.5:3306 has an error, retrying. caused by  java.lang.IllegalArgumentException: Invalid charset id: 255         at com.taobao.tddl.dbsync.binlog.CharsetConversion.getEntry(CharsetConversion.java:41) ~[canal.parse.dbsync-1.1.2.jar:na]         at com.taobao.tddl.dbsync.binlog.CharsetConversion.getJavaCharset(CharsetConversion.java:299) ~[canal.parse.dbsync-1.1.2.jar:na]         at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.&lt;init&gt;(QueryLogEvent.java:503) ~[canal.parse.dbsync-1.1.2.jar:na]         at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:168) ~[canal.parse.dbsync-1.1.2.jar:na]         at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:111) ~[canal.parse.dbsync-1.1.2.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.seek(MysqlConnection.java:137) ~[canal.parse-1.1.2.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:743) ~[canal.parse-1.1.2.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findEndPositionWithMasterIdAndTimestamp(MysqlEventParser.java:392) ~[canal.parse-1.1.2.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:447) ~[canal.parse-1.1.2.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:366) ~[canal.parse-1.1.2.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:186) ~[canal.parse-1.1.2.jar:na]         at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181] 2018-12-26 11:36:30.205 [destination = chess , address = mysql/10.0.0.5:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:chess[java.lang.IllegalArgumentException: Invalid charset id: 255         at com.taobao.tddl.dbsync.binlog.CharsetConversion.getEntry(CharsetConversion.java:41)         at com.taobao.tddl.dbsync.binlog.CharsetConversion.getJavaCharset(CharsetConversion.java:299)         at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.&lt;init&gt;(QueryLogEvent.java:503)         at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:168)         at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:111)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.seek(MysqlConnection.java:137)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findAsPerTimestampInSpecificLogFile(MysqlEventParser.java:743)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findEndPositionWithMasterIdAndTimestamp(MysqlEventParser.java:392)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:447)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:366)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:186)         at java.lang.Thread.run(Thread.java:748) ]  ### Steps to reproduce 因为生产上数据库8.0.3，故从1.1.1升级到1.1.2,结果报这样的错，请问是哪里配置出问题？另外mysql在docker环境下，禁止ping</body>
		<created>2018-12-26 03:42:40</created>
		<closed>2018-12-28 05:47:18</closed>
	</bug>
	<bug>
		<id>1306</id>
		<title>同步时报类型转换错误，求大大解答</title>
		<body>### environment  * canal version：canal.deployer-1.1.0 * mysql version：5.7  ### Issue Description 日志报错 ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:assetSync[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: java.lang.ClassCastException: com.alibaba.fastsql.sql.visitor.SQLASTOutputVisitor cannot be cast to com.alibaba.fastsql.sql.dialect.mysql.visitor.MySqlASTVisitor Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: java.lang.ClassCastException: com.alibaba.fastsql.sql.visitor.SQLASTOutputVisitor cannot be cast to com.alibaba.fastsql.sql.dialect.mysql.visitor.MySqlASTVisitor Caused by: java.lang.ClassCastException: com.alibaba.fastsql.sql.visitor.SQLASTOutputVisitor cannot be cast to com.alibaba.fastsql.sql.dialect.mysql.visitor.MySqlASTVisitor at com.alibaba.fastsql.sql.dialect.mysql.ast.expr.MySqlOrderingExpr.accept0(MySqlOrderingExpr.java:64) at com.alibaba.fastsql.sql.ast.SQLObjectImpl.accept(SQLObjectImpl.java:51) at com.alibaba.fastsql.sql.ast.SQLObjectImpl.output(SQLObjectImpl.java:92) at com.alibaba.fastsql.sql.ast.SQLObjectImpl.output(SQLObjectImpl.java:77) at com.alibaba.fastsql.sql.ast.SQLObjectImpl.toString(SQLObjectImpl.java:99) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.getSqlName(MemoryTableMeta.java:251) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.processTableElement(MemoryTableMeta.java:228) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.parse(MemoryTableMeta.java:155) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.find(MemoryTableMeta.java:110) at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.find(DatabaseTableMeta.java:98) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:162) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:889) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEventForTableMeta(LogEventConvert.java:484) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:252) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlMultiStageCoprocessor$SimpleParserStage.onEvent(MysqlMultiStageCoprocessor.java:222) at com.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ]   ### Steps to reproduce  ### Expected behaviour  ### Actual behaviour </body>
		<created>2018-12-17 10:11:08</created>
		<closed>2018-12-19 03:45:10</closed>
	</bug>
	<bug>
		<id>1168</id>
		<title>TableMeta Error</title>
		<body>issues #980 描述同样的问题，得到的结果是已经在v1.1.1版本解决该Bug。 而我当前的版本就是v1.1.1，依然出现该问题  2018-11-20 17:25:19.708 [[scheduler-table-meta-snapshot]] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue, show create table ddl:CREATE TABLE `vs_order_goods_hold` (   `pk_id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键',   `fk_order_id` int(11) NOT NULL COMMENT '订单ID（表：vs_order）',   `fk_store_id` int(11) NOT NULL COMMENT '店铺ID（表：vs_store）',   `fk_goods_id` int(11) NOT NULL COMMENT '商品ID（表：vs_store_goods）',   `fk_specgds_id` int(11) NOT NULL COMMENT '规格ID（表：esl_gds_specgds）',   `hold_num` int(10) NOT NULL COMMENT '订单规格商品数量',   `is_archiving` int(1) NOT NULL COMMENT '是否归档；0：false ，1：true',   `collage_order` int(1) DEFAULT NULL COMMENT '是否拼团订单商品（0：否，1：是）',   `fk_collage_act_id` int(11) DEFAULT NULL COMMENT '拼团活动ID（表：esl_sale_activity）',   `remark` varchar(50) DEFAULT NULL COMMENT '备注',   `create_user` int(11) NOT NULL COMMENT '创建者用户ID（表：vs_member）',   `create_time` datetime NOT NULL COMMENT '创建时间',   `update_user` int(11) DEFAULT NULL COMMENT '更新者用户ID（表：vs_member）',   `update_time` datetime DEFAULT NULL COMMENT '更新时间',   PRIMARY KEY (`pk_id`) ) ENGINE=InnoDB AUTO_INCREMENT=3540 DEFAULT CHARSET=utf8 COMMENT='订单占有商品库存' , compare failed .   db : TableMeta [schema=phs_erp_test, table=vs_order_goods_hold, fileds= FieldMeta [columnName=pk_id, columnType=int(11), nullable=false, key=true, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=fk_order_id, columnType=int(11), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=fk_store_id, columnType=int(11), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=fk_goods_id, columnType=int(11), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=fk_specgds_id, columnType=int(11), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=hold_num, columnType=int(10), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=is_archiving, columnType=int(1), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=collage_order, columnType=int(1), nullable=true, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=fk_collage_act_id, columnType=int(11), nullable=true, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=remark, columnType=varchar(50), nullable=true, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=create_user, columnType=int(11), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=create_time, columnType=datetime, nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=update_user, columnType=int(11), nullable=true, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=update_time, columnType=datetime, nullable=true, key=false, defaultValue=null, extra=null, unique=false] ]   mem : TableMeta [schema=phs_erp_test, table=vs_order_goods_hold, fileds= FieldMeta [columnName=pk_id, columnType=int(11), nullable=false, key=true, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=fk_order_id, columnType=int(11), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=fk_store_id, columnType=int(11), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=fk_goods_id, columnType=int(11), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=fk_specgds_id, columnType=int(11), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=hold_num, columnType=int(10), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=is_archiving, columnType=int(1), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=remark, columnType=varchar(50), nullable=true, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=create_user, columnType=int(11), nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=create_time, columnType=datetime, nullable=false, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=update_user, columnType=int(11), nullable=true, key=false, defaultValue=null, extra=null, unique=false] FieldMeta [columnName=update_time, columnType=datetime, nullable=true, key=false, defaultValue=null, extra=null, unique=false] ]</body>
		<created>2018-11-21 01:33:37</created>
		<closed>2018-11-25 13:02:21</closed>
	</bug>
	<bug>
		<id>1100</id>
		<title>DatabaseTableMeta#compareTableMetaDbAndMemory return false</title>
		<body>mysql version: 5.7.17-log  建表的DDL如下, int和bigint没带长度 ~~~sql create table test( id int NOT NULL, id2 bigint NOT NULL ) ~~~  执行show create table test发现mysql默认给int和bigint带上了长度(`mysql也许也会给其它类型加上默认的长度`) ~~~sql CREATE TABLE `test` (   `id` int(11) NOT NULL,   `id2` bigint(20) NOT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 ~~~   结果: 在对比内存与db的表结构时返回了false</body>
		<created>2018-11-05 09:07:01</created>
		<closed>2018-11-08 05:58:16</closed>
	</bug>
	<bug>
		<id>1087</id>
		<title>版本1.1.1 docker启动失败</title>
		<body>使用1.1.1直接投递消息到kafka方案，按照wiki文档修改相关参数，成功。 然而：同样的配置，使用docker方式容器启动不成功，查看canal以及instance日志，无任何错误信息，把日志级别开到debug一行一行查看日志，也未发现异常情况。 有其他大神遇到类似情况么？搞了一天了，实在是没辙了。</body>
		<created>2018-11-01 07:49:36</created>
		<closed>2018-11-09 05:59:56</closed>
	</bug>
	<bug>
		<id>1081</id>
		<title>10.1.22-MariaDB版本数据库 journalName乱码</title>
		<body>你好，我使用10.1.22-MariaDB和1.1.1版本canal，启动后报如下错误 2018-11-01 09:40:42.663 [destination = cloud , address = /192.168.1.21:3306 , EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - prepare to find start position just last position  {"identity":{"slaveId":-1,"sourceAddress":{"address":"192.168.1.21","port":3306}},"postion":{"gtid":"","included":false,**"journalName":"mysql-bin.000607Æ\u009E´U"**,"position":91598031,"serverId":1,"timestamp":1541028888000}} 2018-11-01 09:40:42.674 [destination = cloud , address = /192.168.1.21:3306 , EventParser] WARN  c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - find start position : EntryPosition[included=false,journalName=mysql-bin.000607Æ´U,position=91598031,serverId=1,gtid=,timestamp=1541028888000] 2018-11-01 09:40:42.690 [destination = cloud , address = /192.168.1.21:3306 , EventParser] ERROR c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O error while reading from client socket java.io.IOException: Received error packet: errno = 1236, sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) ~[canal.parse-1.1.1.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:216) [canal.parse-1.1.1.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:252) [canal.parse-1.1.1.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-11-01 09:40:42.691 [destination = cloud , address = /192.168.1.21:3306 , EventParser] ERROR c.a.o.c.p.inbound.mysql.rds.RdsBinlogEventParserProxy - dump address /192.168.1.21:3306 has an error, retrying. caused by  java.io.IOException: Received error packet: errno = 1236, sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) ~[canal.parse-1.1.1.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:216) ~[canal.parse-1.1.1.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:252) ~[canal.parse-1.1.1.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_161] 2018-11-01 09:40:42.692 [destination = cloud , address = /192.168.1.21:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:cloud[java.io.IOException: Received error packet: errno = 1236, sqlstate = HY000 errmsg = Could not find first log file name in binary log index file at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:102) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:216) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:252) at java.lang.Thread.run(Thread.java:748) ] 看到是journalName有乱码，请问怎么解决</body>
		<created>2018-11-01 01:42:01</created>
		<closed>2018-11-01 08:05:15</closed>
	</bug>
	<bug>
		<id>1046</id>
		<title>列类型是tinyint(1) unsigned，但实际数据值大于127时，canal获取的列值错误</title>
		<body>列类型是tinyint(1) unsigned，但实际数据值大于127，在128～255之间，canal是按照boolean来处理的，直接转化成string，值变成了负数。在LogEventConvert.java的674行，建议修改如下(未充分测试)              // if (isSingleBit &amp;&amp; javaType == Types.TINYINT) {             //     javaType = Types.BIT;             // }             if (buffer.isNull()) {                 columnBuilder.setIsNull(true);             } else {                 final Serializable value = buffer.getValue();                  if (isSingleBit &amp;&amp; javaType == Types.TINYINT &amp;&amp; ((Number) value).intValue() &gt;= 0) {                     javaType = Types.BIT;                 }                 //...  https://github.com/alibaba/canal/blob/master/parse/src/main/java/com/alibaba/otter/canal/parse/inbound/mysql/dbsync/LogEventConvert.java </body>
		<created>2018-10-26 08:58:03</created>
		<closed>2018-10-26 13:50:36</closed>
	</bug>
	<bug>
		<id>1001</id>
		<title>使用 docker 部署 canal-server 无法使用 docker-restart 命令</title>
		<body>- 现象 当使用 docker 部署官方 canal-server 镜像（canal/canal-server:v1.1.0）并成功启动后，调用 ```shell docker restart ``` 会有以下错误信息： ```log mv: cannot stat `/home/admin/canal-server/conf/example': No such file or directory ```  - 原因 检查 docker 启动脚本，发现 https://github.com/alibaba/canal/blob/master/docker/image/admin/app.sh 的 84-92 行引发的错误： ```shell destination=`perl -le 'print $ENV{"canal.destinations"}'` if [[ "$destination" =~ ',' ]]; then     echo "multi destination:$destination is not support"     exit 1; else     if [ "$destination" != "" ] &amp;&amp; [ "$destination" != "example" ] ; then         mv /home/admin/canal-server/conf/example /home/admin/canal-server/conf/$destination     fi  fi ``` 这里的行为是每次启动时，如果指定了 destination 且 destination 的值不是 example，会把 ```path /home/admin/canal-server/conf/example ``` 目录重命名为指定的 destination 值。 由于我指定了 destination 且值不是 example，所以这个行为导致了执行 ```shell docker restart ``` 时，由于容器内已经没有 example 文件夹，所以容器无法启动。  - 问题 各位大佬是如何解决这个问题的？自己打镜像还是有其他姿势？</body>
		<created>2018-10-12 09:10:47</created>
		<closed>2018-10-16 05:34:29</closed>
	</bug>
	<bug>
		<id>980</id>
		<title>table meta 错误</title>
		<body> ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue, show create table ddl:CREATE TABLE `partner_organization_data` (   `id` int(11) NOT NULL AUTO_INCREMENT,   `organizationName` varchar(250) DEFAULT NULL,   `organizationId` varchar(250) NOT NULL,   `organizationLogo` varchar(500) DEFAULT NULL COMMENT '机构logo',   `organizationUrl` varchar(500) DEFAULT NULL COMMENT '机构地址',   `isDelete` tinyint(4) NOT NULL DEFAULT '0' COMMENT '是否删除0是删除，1是不删除',   `isOnline` tinyint(4) NOT NULL DEFAULT '0' COMMENT '是否上线，0是不上线，1是上线',   `organizationLogoh5_2` varchar(500) DEFAULT NULL COMMENT 'h5  第二个logo',   `organizationLogoh5` varchar(500) DEFAULT NULL COMMENT 'h5机构logo',   `organizationUrlh5` varchar(500) DEFAULT NULL COMMENT 'h5机构url',   PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8 , compare failed .   db : TableMeta [schema=operation, table=partner_organization_data, fileds=         FieldMeta [columnName=id, columnType=int(11), nullable=false, key=true, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationName, columnType=varchar(250), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationId, columnType=varchar(250), nullable=false, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationLogo, columnType=varchar(500), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationUrl, columnType=varchar(500), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=isDelete, columnType=tinyint(4), nullable=false, key=false, defaultValue=0, extra=null, unique=false]         FieldMeta [columnName=isOnline, columnType=tinyint(4), nullable=false, key=false, defaultValue=0, extra=null, unique=false]         FieldMeta [columnName=organizationLogoh5_2, columnType=varchar(500), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationLogoh5, columnType=varchar(500), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationUrlh5, columnType=varchar(500), nullable=true, key=false, defaultValue=null, extra=null, unique=false] ]   mem : TableMeta [schema=operation, table=partner_organization_data, fileds=         FieldMeta [columnName=id, columnType=int(11), nullable=false, key=true, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationName, columnType=varchar(250), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationId, columnType=varchar(250), nullable=false, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationLogo, columnType=varchar(500), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationUrl, columnType=varchar(500), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=isDelete, columnType=tinyint(4), nullable=false, key=false, defaultValue=0, extra=null, unique=false]         FieldMeta [columnName=isOnline, columnType=tinyint(4), nullable=false, key=false, defaultValue=0, extra=null, unique=false]         FieldMeta [columnName=organizationLogoh5_2, columnType=varchar(500), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationLogoh5_2, columnType=varchar(500), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationLogoh5, columnType=varchar(500), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=organizationUrlh5, columnType=varchar(500), nullable=true, key=false, defaultValue=null, extra=null, unique=false] ] 2018-09-30 15:53:41.988 [[scheduler-table-meta-snapshot]] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - compare failed , check log   以上是错误日志, 于两天前增加了一个数据列 organizationLogoh5_2 今天发生的错误.mem中出现了两个organizationLogoh5_2 </body>
		<created>2018-09-30 08:05:27</created>
		<closed>2018-10-16 08:46:13</closed>
	</bug>
	<bug>
		<id>968</id>
		<title>并行解析下，数据库一直连不上导致OOM异常</title>
		<body>版本 v1.1.1-alpha 1 heapdump: ![image](https://user-images.githubusercontent.com/33280738/46004767-435d5000-c0e6-11e8-942d-1b283879a799.png) </body>
		<created>2018-09-25 09:13:23</created>
		<closed>2018-09-29 05:50:25</closed>
	</bug>
	<bug>
		<id>890</id>
		<title>并行解析 + Gtid .没有初始化LogContext中的gtidSet</title>
		<body>没有初始化LogContext中的gtidSet     public final void putGtid(GtidLogEvent logEvent) {         if (logEvent != null) {             String gtid = logEvent.getSid().toString() + ":" + logEvent.getGno();             if (gtidSet == null) {                 gtid = logEvent.getSid().toString() + ":1-" + logEvent.getGno();                 gtidSet = MysqlGTIDSet.parse(gtid);             }             gtidSet.update(gtid);         }     } event中的当前server的gtid会覆盖掉zk cursor中的历史gtidSet（若干个server:start-end 组合） 下次再重启，dump报错。 </body>
		<created>2018-08-27 10:53:05</created>
		<closed>2018-09-04 02:50:36</closed>
	</bug>
	<bug>
		<id>850</id>
		<title>alter语句无法解析</title>
		<body>正常的mysql alter语句，server端解析出错。 无法跳过，目前想到的暂时解决办法是手动在目标库执行，然后把offset往后移。  2018-08-14 16:53:22.500 [destination =xxxx , address = /xxxx , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : ALTER TABLE `loan_withdraw_record` ADD COLUMN `remark` varchar(255) DEFAULT NULL COMMENT '备注信息' AFTER `is_remind_limit`,ALGORITHM=inplace,LOCK=NONE com.alibaba.fastsql.sql.parser.ParserException: syntax error, expect TABLES or TABLE, actual EQ, pos 143, line 1, column 143, token =         at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseStatementListDialect(MySqlStatementParser.java:863) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:483) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[fastsql-2.0.0_preview_371.jar:2.0.0_preview_371]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:104) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:228) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:265) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:126) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:68) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:345) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:187) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:154) [canal.parse-1.0.26-SNAPSHOT.jar:na] SHTERM: session timeoutotter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:227) [canal.parse-1.0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]</body>
		<created>2018-08-17 07:32:49</created>
		<closed>2018-10-29 04:47:11</closed>
	</bug>
	<bug>
		<id>818</id>
		<title>HA模式下，canal.instance.tsdb.enable=true，数据库进行主从切换后，canal的master与standby 切换后，会报如下异常。目前我使用的解决方式是将canal.instance.tsdb.enable=false，就可以进行正常的切换了。</title>
		<body>HA模式下，canal.instance.tsdb.enable=true，数据库进行主从切换后，canal的master与standby 切换后，会报如下异常。目前我使用的解决方式是将canal.instance.tsdb.enable=false，就可以进行正常的切换了。  2018-08-07 10:44:48.772 [destination = orderfailover , address = /10.8.132.135:3306 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::1532919362000 2018-08-07 10:44:48.780 [destination = orderfailover , address = /10.8.132.135:3306 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn't find the corresponding binlog files from mysql-bin.000020 to mysql-bin.000021 2018-08-07 10:44:48.782 [destination = orderfailover , address = /10.8.132.135:3306 , EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.8.132.135:3306 has an error, retrying. caused by com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for orderfailover 2018-08-07 10:44:48.782 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException, url: , errorCode 0, state null java.sql.SQLException: connect error, url , driverClass org.h2.Driver         at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9]         at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:48.782 [destination = orderfailover , address = /10.8.132.135:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:orderfailover[com.alibaba.otter.canal.parse.exception.CanalParseException: can't find start position for orderfailover ] 2018-08-07 10:44:49.282 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException, url: , errorCode 0, state null java.sql.SQLException: connect error, url , driverClass org.h2.Driver         at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9]         at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:49.783 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException, url: , errorCode 0, state null java.sql.SQLException: connect error, url , driverClass org.h2.Driver         at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9]         at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:50.283 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException, url: , errorCode 0, state null java.sql.SQLException: connect error, url , driverClass org.h2.Driver         at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9]         at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:50.783 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException, url: , errorCode 0, state null java.sql.SQLException: connect error, url , driverClass org.h2.Driver         at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1582) ~[druid-1.1.9.jar:1.1.9]         at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2466) ~[druid-1.1.9.jar:1.1.9] 2018-08-07 10:44:51.283 [Druid-ConnectionPool-Create-1877078260] ERROR com.alibaba.druid.pool.DruidDataSource - create connection SQLException, url: , errorCode 0, state null  </body>
		<created>2018-08-07 09:39:37</created>
		<closed>2018-08-07 10:53:18</closed>
	</bug>
	<bug>
		<id>776</id>
		<title>[v1.0.26.alpha4] decode event时报limit exceed错误</title>
		<body>![image](https://user-images.githubusercontent.com/8179551/43183817-099b0e50-9019-11e8-8339-2d922e48b9ed.png) decode event时报limit exceed错误 应该是和这个commit有关 https://github.com/alibaba/canal/commit/89726a636530b73a6b97cecc2b5bcee4fb464f86 </body>
		<created>2018-07-25 06:46:47</created>
		<closed>2018-07-26 07:15:51</closed>
	</bug>
	<bug>
		<id>736</id>
		<title>com.alibaba.fastsql.sql.parser.ParserException</title>
		<body>场景：TSDB开启 fastsql版本：371 升级到371版本的fastsql后，目前遇见两种类型exception.  一： Student-service 2018-07-05 17:10:13.017  WARN ???? [6 , EventParser] c.a.o.c.p.i.m.t.MemoryTableMeta          : [] parse faield : ALTER TABLE `platform`.`notice`  CHANGE COLUMN `content` `content` VARCHAR(3000) CHARACTER SET 'utf8mb4' COLLATE 'utf8mb4_unicode_ci' NOT NULL DEFAULT '未填写' COMMENT '默认'  com.alibaba.fastsql.sql.parser.ParserException at com.alibaba.fastsql.sql.parser.SQLExprParser.parseCharTypeRest(SQLExprParser.java:2910) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2785) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.parser.SQLExprParser.parseDataType(SQLExprParser.java:2666) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlExprParser.parseColumn(MySqlExprParser.java:463) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlterTable(MySqlStatementParser.java:4315) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.dialect.mysql.parser.MySqlStatementParser.parseAlter(MySqlStatementParser.java:3307) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:248) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[Student-service.jar!/:?]  二： Student-service 2018-07-05 17:32:42.210  WARN ???? [6 , EventParser] c.a.o.c.p.i.m.t.MemoryTableMeta          : [] parse faield : ALTER TABLE `student` DROP COLUMN `display_name`, DROP COLUMN `system`, MODIFY COLUMN `id`  bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT FIRST , CHANGE COLUMN `name` `student_name`  varchar(255) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL DEFAULT '' COMMENT '学生名' AFTER `id`, MODIFY COLUMN `description`  varchar(255) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL DEFAULT '' COMMENT '学校描述' AFTER `student_name`, MODIFY COLUMN `created_at`  timestamp NOT NULL AFTER `description`, MODIFY COLUMN `updated_at`  timestamp NOT NULL AFTER `created_at`, CHANGE COLUMN `creator` `created_by`  varchar(50) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL AFTER `updated_at`, CHANGE COLUMN `updated_name` `updated_by`  varchar(50) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL AFTER `created_by`, ADD COLUMN `is_deleted`  tinyint(1) NOT NULL DEFAULT 0 COMMENT '删除标记，0，未删除，1，删除' AFTER `description`  com.alibaba.fastsql.sql.parser.ParserException: syntax error, error in :'MODIFY COLUMN `id`  bigint(20) UNSIGN, pos 86, line 4, column 9, token COLUMN at com.alibaba.fastsql.sql.parser.SQLParser.printError(SQLParser.java:361) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:498) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.SQLUtils.parseStatements(SQLUtils.java:500) ~[Student-service.jar!/:?] at com.alibaba.fastsql.sql.repository.SchemaRepository.console(SchemaRepository.java:412) ~[Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:72) [Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:104) [Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:228) [Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:265) [Student-service.jar!/:?] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:126) [Student-service.jar!/:?]</body>
		<created>2018-07-05 11:10:07</created>
		<closed>2018-07-18 11:31:56</closed>
	</bug>
	<bug>
		<id>729</id>
		<title>update 语句为什么是ddl ？</title>
		<body> ![image](https://user-images.githubusercontent.com/5965173/42151092-603695a8-7e0e-11e8-8386-8920b4bc4d70.png)   如图。</body>
		<created>2018-07-02 06:01:54</created>
		<closed>2018-07-02 06:35:27</closed>
	</bug>
	<bug>
		<id>697</id>
		<title>ClientRunningMonitor shutdown delayExector threadpool</title>
		<body>Version：1.0.26.alpha2 当client在cluster模式下发生ha切换时，com.alibaba.otter.canal.client.impl.running.ClientRunningMonitor 会submit job到delayExector，必须在stop方法里执行delayExector.shutdown()，否则不能优雅shutdown应用  一会儿提交pr，please assign the issue to me. Thanx.</body>
		<created>2018-06-14 13:16:22</created>
		<closed>2018-06-15 05:25:11</closed>
	</bug>
	<bug>
		<id>657</id>
		<title>提出一个关于MemoryMetaManager优化的建议</title>
		<body>在项目运行启动订阅时有遇到一个异常(项目采用的是CanalServerWithEmbedded内嵌，关键的配置MetaMode.MIXED &amp; IndexMode.MEMORY_META_FAILBACK)，栈输出如下：  ``` dump address /xxx.xx.xx.xxx:3306 has an error, retrying. caused by   java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901)  at java.util.ArrayList$Itr.next(ArrayList.java:851)  at com.alibaba.otter.canal.parse.index.MetaLogPositionManager.getLatestIndexBy(MetaLogPositionManager.java:56)  at com.alibaba.otter.canal.parse.index.FailbackLogPositionManager.getLatestIndexBy(FailbackLogPositionManager.java:68)  at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:567)  at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:509) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:167)  at java.lang.Thread.run(Thread.java:745)  ```  看代码发现CanalServerWithEmbedded中的subscribe方法中有调用MetaManager的**subscribe**方法，最终会调用到MemoryMetaManager的subscribe方法，在得到destinations中key对应的list后，修改list（向list里add元素）。代码:  ```     public synchronized void subscribe(ClientIdentity clientIdentity) throws CanalMetaManagerException {         List&lt;ClientIdentity&gt; clientIdentitys = destinations.get(clientIdentity.getDestination());          if (clientIdentitys.contains(clientIdentity)) {             clientIdentitys.remove(clientIdentity);         }          clientIdentitys.add(clientIdentity);     } ```  而MemoryMetaManager类中还有一个方法**listAllSubscribeInfo**会直接返回destinations中key对应的list。代码：  ```     public synchronized List&lt;ClientIdentity&gt; listAllSubscribeInfo(String destination) throws CanalMetaManagerException {         return destinations.get(destination);     } ```  我遇到的问题是在MetaLogPositionManager的getLatestIndexBy方法遍历metaManager.listAllSubscribeInfo调用结果时，业务代码线程调用了subscribe方法修改了遍历的list集合，导致了ConcurrentModificationException。  ```     public LogPosition getLatestIndexBy(String destination) {         List&lt;ClientIdentity&gt; clientIdentities = metaManager.listAllSubscribeInfo(destination);         LogPosition result = null;         if (!CollectionUtils.isEmpty(clientIdentities)) {             // 尝试找到一个最小的logPosition             for (ClientIdentity clientIdentity : clientIdentities) {  //在此处遍历时链表被其它线程修改                 LogPosition position = (LogPosition) metaManager.getCursor(clientIdentity);                 if (position == null) {                     continue;                 }                  if (result == null) {                     result = position;                 } else {                     result = CanalEventUtils.min(result, position);                 }             }         }          return result;     } ```  这里感觉在MemoryMetaManager中的listAllSubscribeInfo方法有比较大的隐患，外部拿到引用以后可能在不同的线程中做遍历或者修改。 如果这里每次都拷贝一个新list返回的话不知道对效率上的损失是否太大，如果不希望过多拷贝的话可以返回unmodifiableList，阻止其他类对这个集合的修改，但是这样在其它类遍历这个集合时还是可能出现我这种ConcurrentModificationException。  想问一下你对这个问题是怎么考虑的呢 @agapple </body>
		<created>2018-05-27 14:07:10</created>
		<closed>2018-06-05 08:04:26</closed>
	</bug>
	<bug>
		<id>641</id>
		<title>表结构解析失败，unknow column</title>
		<body>tag: canal-1.0.26-preview-2 表中有一列名：conf_key  2018-05-17 13:28:36.412 [destination = xxxxx , address = xxxxx , EventParser] ERROR c.a.otter.canal.p arse.inbound.mysql.MysqlEventParser - dump address /10.4.217.125:5002 has an error, retrying. caused by java.lang.RuntimeException: unknow column : `conf_key`(8)         at com.alibaba.otter.canal.parse.inbound.TableMeta.getFieldMetaByName(TableMeta.java:74) ~[canal.parse-1.0.26-SN APSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.processTableElement(MemoryTableMeta.java:227 ) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.parse(MemoryTableMeta.java:153) ~[canal.pars e-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.find(MemoryTableMeta.java:108) ~[canal.parse -1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.compareTableMetaDbAndMemory(DatabaseTableM eta.java:289) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.applySnapshotToDB(DatabaseTableMeta.java:2 51) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:129) ~[can al.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParse r.java:72) ~[canal.parse-1.0.26-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) ~[canal.parse-1 .0.26-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]</body>
		<created>2018-05-17 05:45:42</created>
		<closed>2018-06-05 07:41:31</closed>
	</bug>
	<bug>
		<id>639</id>
		<title>异常：fetch failed by table meta，版本1.0.26，tsdb.enable=true</title>
		<body>原sql是对表card_record 进行了两次alter操作 alter table card_record modify column customization_id bigint unsigned NOT NULL COMMENT '定制id' | 查看详情 alter table card_record modify column upgraded_customization_id bigint unsigned NOT NULL COMMENT '升级后定制id' | 查看详情  线上一次alter操作步骤是： 1. DROP TABLE IF EXISTS `_card_record_gho` 2. DROP TABLE IF EXISTS `_card_record_del` 3. create table `yushitai_test`.`_card_record_gho` like `yushitai_test`.`card_record` 4. alter table `yushitai_test`.`_card_record_gho`  modify column customization_id bigint unsigned NOT NULL COMMENT 'ŚģöŚą∂id' 5. insert 数据到_card_record_gho 6. rename table `yushitai_test`.`card_record` to `yushitai_test`.`_card_record_del` `yushitai_test`.`_card_record_gho` to `yushitai_test`.`card_record`  手动设置位点到两条alter操作时间之前，重新消费时，当执行第二次alter 操作时，到步骤5时，会出现exception： fetch failed by table meta:`yushitai_test`.`_card_record_gho`  2018-05-14 15:32:44.078 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_gho` /* generated by server */ 2018-05-14 15:32:44.078 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_del` /* generated by server */ 2018-05-14 15:32:44.078 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_ghc` /* generated by server */ 2018-05-14 15:32:44.079 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_ghc` ( id bigint auto_increment, last_update timestamp not null DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, hint varchar(64) charset ascii not null, value varchar(255) charset ascii not null, primary key(id), unique key hint_uidx(hint) ) auto_increment=256 2018-05-14 15:32:44.079 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_gho` like `yushitai_test`.`card_record` 2018-05-14 15:32:44.080 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : alter /* gh-ost */ table `yushitai_test`.`_card_record_gho`  modify column customization_id bigint unsigned NOT NULL COMMENT 'ŚģöŚą∂id' 2018-05-14 15:32:44.085 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_del` ( id int auto_increment primary key ) engine=InnoDB comment='ghost-cut-over-sentry' 2018-05-14 15:32:44.090 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_del` /* generated by server */ 2018-05-14 15:32:44.090 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : rename /* gh-ost */ table `yushitai_test`.`card_record` to `yushitai_test`.`_card_record_del`, `yushitai_test`.`_card_record_gho` to `yushitai_test`.`card_record` 2018-05-14 15:32:44.091 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_ghc` /* generated by server */ 2018-05-14 15:32:44.104 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : rename table `_card_record_del` to _card_record_del_bak20180508125310 2018-05-14 15:32:45.093 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_gho` /* generated by server */ 2018-05-14 15:32:45.094 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_del` /* generated by server */ 2018-05-14 15:32:45.094 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : DROP TABLE IF EXISTS `_card_record_ghc` /* generated by server */ 2018-05-14 15:32:45.096 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_ghc` ( id bigint auto_increment, last_update timestamp not null DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, hint varchar(64) charset ascii not null, value varchar(255) charset ascii not null, primary key(id), unique key hint_uidx(hint) ) auto_increment=256 2018-05-14 15:32:45.096 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : create /* gh-ost */ table `yushitai_test`.`_card_record_gho` like `yushitai_test`.`card_record` 2018-05-14 15:32:45.097 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - dup apply for sql : alter /* gh-ost */ table `yushitai_test`.`_card_record_gho`  modify column upgraded_customization_id bigint unsigned NOT NULL COMMENT 'ŚćáÁļßŚźéŚģöŚą∂id' 2018-05-14 15:32:45.103 [destination = yushitai_test , address = /10.32.200.228:5002 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error , last position : [EntryPosition[included=false,journalName=mysql-bin.000056,position=286998741,serverId=32114196,timestamp=1525755246000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`yushitai_test`.`_card_record_gho` Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: fetch failed by table meta:`yushitai_test`.`_card_record_gho` Caused by: java.io.IOException: ErrorPacket [errorNumber=1146, fieldCount=-1, message=Table 'yushitai_test._card_record_gho' doesn't exist, sqlState=42S02, sqlStateMarker=#]  with command: show create table `yushitai_test`.`_card_record_gho` at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.query(MysqlQueryExecutor.java:61) ~[canal.parse.driver-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:94) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:167) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:152) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]</body>
		<created>2018-05-14 08:20:43</created>
		<closed>2018-06-08 02:57:14</closed>
	</bug>
	<bug>
		<id>631</id>
		<title>MySQL驱动包支持Boolean类型，canal不支持boolean类型转换</title>
		<body>MySQL的驱动包中对tinyint的长度为1，对应sqlType=-7,转化为Java布尔类型，参考MySQL驱动源码  ![image](https://user-images.githubusercontent.com/9798724/39748475-bff67406-52e2-11e8-9233-21df121524b9.png)  而Canal对对tinyint的长度为1，对应sqlType=-6，无法处理为Java的布尔类型</body>
		<created>2018-05-08 09:11:20</created>
		<closed>2018-06-08 02:45:31</closed>
	</bug>
	<bug>
		<id>610</id>
		<title>DDL语句的comment中文出现乱码</title>
		<body>MySQL中，charset为 utf8mb4，collation为 utf8mb4_general_ci  测试过程中建表语句如下： ```sql CREATE TABLE `xxx` (   `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键id，自增',   `run_time` int(13) NOT NULL DEFAULT '0' COMMENT '运行总时长-秒',   `init_time` int(13) NOT NULL DEFAULT '0' COMMENT '初始化时长-秒',   `simulation_time` int(13) NOT NULL DEFAULT '0' COMMENT '模拟计算耗时-秒',   `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',   `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',   PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='hadoop处理能力标准监控程序运行时长'; ```  Canal server接收后，得到的SQL语句如下： ```sql create table `xxx` ( `id` int(11) NOT NULL auto_increment COMMENT 'šłĽťĒģidÔľĆŤá™ŚĘě', `run_time` int(13) NOT NULL DEFAULT 0 COMMENT 'ŤŅźŤ°ĆśÄĽśó∂ťēŅ-Áßí', `init_time` int(13) NOT NULL DEFAULT 0 COMMENT 'ŚąĚŚßčŚĆĖśó∂ťēŅ-Áßí',  `simulation_time` int(13) NOT NULL DEFAULT 0 COMMENT 'ś®°śčüŤģ°ÁģóŤÄóśó∂-Áßí',  `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'ŚąõŚĽļśó∂ťóī', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'śõīśĖįśó∂ťóī', PRIMARY KEY(`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT 'hadoopŚ§ĄÁźÜŤÉĹŚäõś†áŚáÜÁõĎśéßÁ®čŚļŹŤŅźŤ°Ćśó∂ťēŅ'; ```  问题可能的原因，在canal server接收到binlog日志时，会从binlog日志中得到 clientCharset = 45，从而得到 charsetName = MacCentralEurope，再解码得出上述的带中文乱码的SQL https://github.com/alibaba/canal/blob/f46133d1168071741e8e3e4235aa635c5870a976/dbsync/src/main/java/com/taobao/tddl/dbsync/binlog/event/QueryLogEvent.java#L482 https://github.com/alibaba/canal/blob/f46133d1168071741e8e3e4235aa635c5870a976/dbsync/src/main/java/com/taobao/tddl/dbsync/binlog/CharsetConversion.java#L106 ```java         // 这两项定义是否正确？         putEntry(45, "utf8mb4", "utf8mb4_general_ci", "MacCentralEurope");         putEntry(46, "utf8mb4", "utf8mb4_bin", "MacCentralEurope"); ``` </body>
		<created>2018-04-24 16:23:48</created>
		<closed>2018-05-10 11:55:12</closed>
	</bug>
	<bug>
		<id>570</id>
		<title>‘FULLTEXT KEY’  and ‘ADD INDEX USING BTREE’   cause parse error</title>
		<body>v1.0.25 `parse faield : ALTER TABLE `xxxxx` ADD INDEX `idx_order_id` (`order_id`) USING BTREE com.alibaba.druid.sql.parser.ParserException: syntax error, error in :'SING BTREE', expect IDENTIFIER, actual IDENTIFIER pos 132, line 4, column 45, token IDENTIFIER BTREE         at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:421) ~[druid-1.1.6.jar:1.1.6] `   `2018-03-26 09:11:52.244 [destination = kd_caesar_yf , address = test.kuaihuoyun.com/118.178.142.131:3306 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `xxxx` (   `id` int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT '',   ...   FULLTEXT KEY `ft_query_oid` (`query_oid`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='' com.alibaba.druid.sql.parser.ParserException: syntax error, error in :' KEY `ft_query_oid` (`query_oid`) )', expect RPAREN, actual IDENTIFIER pos 2255, line 37, column 16, token IDENTIFIER `ft_query_oid`         at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:284) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.6.jar:1.1.6]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.apply(DatabaseTableMeta.java:104) [canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.apply(TableMetaCache.java:203) [canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseQueryEvent(LogEventConvert.java:194) [canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:107) [canal.parse-1.0.25.jar:na] `</body>
		<created>2018-03-30 07:18:23</created>
		<closed>2018-06-12 05:29:15</closed>
	</bug>
	<bug>
		<id>535</id>
		<title>canal server启动时对uniquekey报错compare failed</title>
		<body>``` 2018-02-25 10:23:39.643 [destination = example , address = rds9h7gi6v2fo2og5202.mysql.rds.aliyuncs.com/100.98.57.68:3306 , EventParser] ERROR c.a.o.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta - pls submit github issue, show create table ddl:CREATE TABLE `center_institution_admin` (   `id` bigint(20) NOT NULL AUTO_INCREMENT,   `account` varchar(20) NOT NULL COMMENT '账号',   `cinst_id` bigint(20) NOT NULL COMMENT '机构id',   `status` int(11) DEFAULT '1' COMMENT '0:关闭,1:开启',   `gmt_create` datetime DEFAULT CURRENT_TIMESTAMP,   `gmt_modify` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,   `name` varchar(50) DEFAULT NULL COMMENT '账号角色名称',   `parent_id` bigint(20) NOT NULL DEFAULT '0' COMMENT '父账号ID',   `permissions` varchar(500) DEFAULT '0' COMMENT '账号权限码',   `remark` varchar(256) DEFAULT NULL,   `default_account` int(11) DEFAULT '0',   PRIMARY KEY (`id`),   UNIQUE KEY `inst_account_unique_index` (`cinst_id`,`account`),   KEY `parent_id_index` (`parent_id`),   KEY `center_institution_admin_account_index` (`account`) ) ENGINE=InnoDB AUTO_INCREMENT=967573674662035553 DEFAULT CHARSET=utf8 COMMENT='中心机构管理账号表' , compare failed .  db : TableMeta [schema=xiaomai, table=center_institution_admin, fileds=         FieldMeta [columnName=id, columnType=bigint(20), nullable=false, key=true, defaultValue=null, extra=auto_increment, unique=false]         FieldMeta [columnName=account, columnType=varchar(20), nullable=false, key=false, defaultValue=null, extra=, unique=false]         FieldMeta [columnName=cinst_id, columnType=bigint(20), nullable=false, key=false, defaultValue=null, extra=, unique=false]         FieldMeta [columnName=status, columnType=int(11), nullable=true, key=false, defaultValue=1, extra=, unique=false]         FieldMeta [columnName=gmt_create, columnType=datetime, nullable=true, key=false, defaultValue=CURRENT_TIMESTAMP, extra=, unique=false]         FieldMeta [columnName=gmt_modify, columnType=datetime, nullable=true, key=false, defaultValue=CURRENT_TIMESTAMP, extra=on update CURRENT_TIMESTAMP, unique=false]         FieldMeta [columnName=name, columnType=varchar(50), nullable=true, key=false, defaultValue=null, extra=, unique=false]         FieldMeta [columnName=parent_id, columnType=bigint(20), nullable=false, key=false, defaultValue=0, extra=, unique=false]         FieldMeta [columnName=permissions, columnType=varchar(500), nullable=true, key=false, defaultValue=0, extra=, unique=false]         FieldMeta [columnName=remark, columnType=varchar(256), nullable=true, key=false, defaultValue=null, extra=, unique=false]         FieldMeta [columnName=default_account, columnType=int(11), nullable=true, key=false, defaultValue=0, extra=, unique=false] ]  mem : TableMeta [schema=xiaomai, table=center_institution_admin, fileds=         FieldMeta [columnName=id, columnType=bigint(20), nullable=false, key=true, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=account, columnType=varchar(20), nullable=false, key=false, defaultValue=null, extra=null, unique=true]         FieldMeta [columnName=cinst_id, columnType=bigint(20), nullable=false, key=false, defaultValue=null, extra=null, unique=true]         FieldMeta [columnName=status, columnType=int(11), nullable=true, key=false, defaultValue=1, extra=null, unique=false]         FieldMeta [columnName=gmt_create, columnType=datetime, nullable=true, key=false, defaultValue=CURRENT_TIMESTAMP, extra=null, unique=false]         FieldMeta [columnName=gmt_modify, columnType=datetime, nullable=true, key=false, defaultValue=CURRENT_TIMESTAMP, extra=null, unique=false]         FieldMeta [columnName=name, columnType=varchar(50), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=parent_id, columnType=bigint(20), nullable=false, key=false, defaultValue=0, extra=null, unique=false]         FieldMeta [columnName=permissions, columnType=varchar(500), nullable=true, key=false, defaultValue=0, extra=null, unique=false]         FieldMeta [columnName=remark, columnType=varchar(256), nullable=true, key=false, defaultValue=null, extra=null, unique=false]         FieldMeta [columnName=default_account, columnType=int(11), nullable=true, key=false, defaultValue=0, extra=null, unique=false] ] ``` 使用的是目前master分支最新代码，https://github.com/alibaba/canal/issues/507 的问题似乎没有解决.  测试表中UK是一个联合索引（`cinst_id`,`account`），因此在单独field上的unique标志应该为false，但mem中的数据对于单独field也为true </body>
		<created>2018-02-25 02:28:09</created>
		<closed>2018-03-12 08:29:14</closed>
	</bug>
	<bug>
		<id>507</id>
		<title>desc tableName 与 show create table tableName 对比不一致</title>
		<body>CREATE TABLE `IM_GROUP_USER` (   `GROUPID` int(11) NOT NULL,   `USERID` int(11) NOT NULL,   `MANA` int(11) DEFAULT NULL,   `NOTE` varchar(200) COLLATE gbk_bin DEFAULT NULL,   UNIQUE KEY `IM_GROUP_USER_INDEX` (`GROUPID`,`USERID`) ) ENGINE=InnoDB DEFAULT CHARSET=gbk COLLATE=gbk_bin , compare failed .   db : TableMeta [schema=imserver, table=IM_GROUP_USER, fileds= FieldMeta [columnName=GROUPID, columnType=int(11), defaultValue=null, nullable=false, key=true] FieldMeta [columnName=USERID, columnType=int(11), defaultValue=null, nullable=false, key=true] FieldMeta [columnName=MANA, columnType=int(11), defaultValue=null, nullable=true, key=false] FieldMeta [columnName=NOTE, columnType=varchar(200), defaultValue=null, nullable=true, key=false] ]   mem : TableMeta [schema=imserver, table=IM_GROUP_USER, fileds= FieldMeta [columnName=GROUPID, columnType=int(11), defaultValue=null, nullable=false, key=false] FieldMeta [columnName=USERID, columnType=int(11), defaultValue=null, nullable=false, key=false] FieldMeta [columnName=MANA, columnType=int(11), defaultValue=null, nullable=true, key=false] FieldMeta [columnName=NOTE, columnType=varchar(200), defaultValue=null, nullable=true, key=false] ]</body>
		<created>2018-01-25 15:23:10</created>
		<closed>2018-02-12 03:16:06</closed>
	</bug>
	<bug>
		<id>494</id>
		<title>启动正常，更新数据后报错</title>
		<body>下载v1.0.26 alpha 1版本，根据网站上的说明修改配置文件；创建数据库：test; 启动程序，在test数据库里的test表插入一条数据，报错如下，错误信息里面显示的其他库里面的表。为什么我更新test库，会报其他库的错误？ [root@localhost canal]# tail -f logs/canal/canal.log  2018-01-17 23:05:53.957 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## set default uncaught exception handler 2018-01-17 23:05:54.022 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## load canal configurations 2018-01-17 23:05:54.023 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server. 2018-01-17 23:05:54.069 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.122.1:11111] 2018-01-17 23:05:54.591 [main] WARN  o.s.beans.GenericTypeAwarePropertyDescriptor - Invalid JavaBean property 'connectionCharset' being accessed! Ambiguous write methods found next to actually used [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.lang.String)]: [public void com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.setConnectionCharset(java.nio.charset.Charset)] 2018-01-17 23:05:54.847 [main] ERROR com.alibaba.druid.pool.DruidDataSource - testWhileIdle is true, validationQuery not set 2018-01-17 23:05:55.127 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 2018-01-17 23:05:55.968 [destination = example , address = /127.0.0.1:3306 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status             2018-01-17 23:06:09.929 [destination = example , address = /127.0.0.1:3306 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `columns_priv` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '',   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '',   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '',   `Table_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '',   `Column_name` char(64) COLLATE utf8_bin NOT NULL DEFAULT '',   `Timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,   `Column_priv` set('Select','Insert','Update','References') CHARACTER SET utf8 NOT NULL DEFAULT '',   PRIMARY KEY (`Host`,`Db`,`User`,`Table_name`,`Column_name`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Column privileges' com.alibaba.druid.sql.parser.ParserException: syntax error, error in :'es') CHARACTER SET utf8 NOT NULL DE', expect RPAREN, actual IDENTIFIER pos 479, line 8, column 62, token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:283) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.repository.SchemaRepository.console(SchemaRepository.java:295) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta.apply(MemoryTableMeta.java:69) ~[canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.dumpTableMeta(DatabaseTableMeta.java:177) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.tsdb.DatabaseTableMeta.rollback(DatabaseTableMeta.java:127) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.AbstractMysqlEventParser.processTableMeta(AbstractMysqlEventParser.java:72) [canal.parse-1.0.26-SNAPSHOT.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:170) [canal.parse-1.0.26-SNAPSHOT.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79] 2018-01-17 23:06:09.930 [destination = example , address = /127.0.0.1:3306 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.tsdb.MemoryTableMeta - parse faield : CREATE TABLE `db` (   `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '',   `Db` char(64) COLLATE utf8_bin NOT NULL DEFAULT '',   `User` char(16) COLLATE utf8_bin NOT NULL DEFAULT '',   `Select_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Insert_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Update_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Delete_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Create_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Drop_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Grant_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `References_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Index_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Alter_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Create_tmp_table_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Lock_tables_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Create_view_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Show_view_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Create_routine_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Alter_routine_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Execute_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Event_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   `Trigger_priv` enum('N','Y') CHARACTER SET utf8 NOT NULL DEFAULT 'N',   PRIMARY KEY (`Host`,`Db`,`User`),   KEY `User` (`User`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='Database privileges' com.alibaba.druid.sql.parser.ParserException: syntax error, error in :''Y') CHARACTER SET utf8 NOT NULL DE', expect RPAREN, actual IDENTIFIER pos 225, line 5, column 31, token IDENTIFIER CHARACTER at com.alibaba.druid.sql.parser.SQLParser.printError(SQLParser.java:283) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLParser.accept(SQLParser.java:292) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlCreateTableParser.parseCreateTable(MySqlCreateTableParser.java:191) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.dialect.mysql.parser.MySqlStatementParser.parseCreate(MySqlStatementParser.java:244) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:159) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.parser.SQLStatementParser.parseStatementList(SQLStatementParser.java:70) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] at com.alibaba.druid.sql.SQLUtils.parseStatements(SQLUtils.java:464) ~[druid-1.1.7-preview_0.jar:1.1.7-preview_0] </body>
		<created>2018-01-17 07:12:29</created>
		<closed>2018-03-12 09:01:38</closed>
	</bug>
	<bug>
		<id>482</id>
		<title>first parse row data failed should execute connector.connect() </title>
		<body>最近凌晨报错这个，导致channel挂起，mysql :5.6.35 ,canal:1.0.25 2018-01-07 00:00:00.085 [destination = sms_log_2 , address = /127.0.0.1:3306 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - ERROR ## parse this event has an error , last position : [EntryPosition[included=false,journalName=mysql-bin.000106,position=583737799,serverId=20563,timestamp=1515254400000]] com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) [canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151] Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.&lt;init&gt;(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) [canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] ... 10 common frames omitted 2018-01-07 00:00:00.087 [destination = sms_log_2 , address = /127.0.0.1:3306 , EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 127.0.0.1/127.0.0.1:3306 has an error, retrying. caused by  com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) ~[guava-18.0.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) ~[canal.parse-1.0.25.jar:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151] Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.&lt;init&gt;(MysqlQueryExecutor.java:30) ~[canal.parse.driver-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) ~[canal.parse-1.0.25.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) ~[canal.parse-1.0.25.jar:na] at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na] at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ~[guava-18.0.jar:na] ... 10 common frames omitted 2018-01-07 00:00:00.088 [destination = sms_log_2 , address = /127.0.0.1:3306 , EventParser] WARN  c.a.o.s.a.i.setl.zookeeper.termin.WarningTerminProcess - nid:3[3:canal:sms_log_2:com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.io.IOException: should execute connector.connect() first at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4832) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMeta(TableMetaCache.java:160) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.getTableMeta(LogEventConvert.java:759) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:428) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:114) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:66) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:337) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:184) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:145) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:220) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: should execute connector.connect() first at com.alibaba.otter.canal.parse.driver.mysql.MysqlQueryExecutor.&lt;init&gt;(MysqlQueryExecutor.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.query(MysqlConnection.java:87) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.getTableMetaByDB(TableMetaCache.java:80) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache.access$000(TableMetaCache.java:30) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:55) at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.TableMetaCache$1.load(TableMetaCache.java:50) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) at com.google.common.cache.LocalCache.get(LocalCache.java:3937) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830) ... 10 more ]    </body>
		<created>2018-01-08 06:50:06</created>
		<closed>2018-01-10 05:50:12</closed>
	</bug>
	<bug>
		<id>449</id>
		<title>当指定的pos刚好是rowdata(TableMap的下一条)数据时，canal server 端会陷入死循环BUG</title>
		<body>测试版本：canal 版本 1.0.24 1. Canal在实现该功能时，通过LogEventConvert.parseRowsEvent 解析event时抛出 TableIdNotFoundException， 但是parseRowsEvent 直接捕获了所有Exception，并重新抛出 CanalParseException，这样AbstractEventParser的parseThread 工作线程获取的一直都是Throwable的异常，无法感知到TableIdNotFoundException，导致陷入死循环；  2. 修改LogEventConvert.parseRowsEvent 使其抛出TableIdNotFoundException后，下次循环线程会感知needTransactionPosition变量已经会true，进入findTransactionBeginPosition方法后，会调用seek方法，但seek方法只解析事务头/尾，那么当前的seek方法中实现的sink的event事件就解析不到(因为rowdata数据被过滤了)，故reDump 一直都不会为true，从而也不会进入从头读取binlog的逻辑内，就又进入了主线程的死循环内。  PS: 个人简单分析，不知是否理解有误，抑或是findTransactionBeginPosition不是为了解决这个场景的?@agapple</body>
		<created>2017-12-13 05:05:42</created>
		<closed>2017-12-27 09:35:20</closed>
	</bug>
	<bug>
		<id>442</id>
		<title>table meta生成snapshot比较方法有问题</title>
		<body>DatabaseTableMeta.applySnapshotToDB()方法中，先获取一个memoryTableMeta的副本tmpMemoryTableMeta，然后再去master库中去对比，然后如果一切正常则持久化snapshot.  但在和master库中ddl做对比时，却meta信息是从memoryTableMeta而不是tmpMemoryTableMeta，见compareTableMetaDbAndMemory()方法。  这样在多线程情况下，会导致数据不一致情况。  fix issue：对比时从tmpMemoryTableMeta中取meta信息。 &gt;&gt;&gt;&gt;&gt;&gt; private boolean compareTableMetaDbAndMemory(MysqlConnection connection, final String schema, final String table) {         TableMeta tableMetaFromMem = memoryTableMeta.find(schema, table);  &lt;&lt;&lt;&lt;&lt;&lt; private boolean compareTableMetaDbAndMemory(MysqlConnection connection, final String schema, final String table，MemoryTableMeta            tmpMemoryTableMeta) {         TableMeta tableMetaFromMem = tmpMemoryTableMeta.find(schema, table); ......  </body>
		<created>2017-12-08 09:45:05</created>
		<closed>2017-12-08 14:57:07</closed>
	</bug>
	<bug>
		<id>440</id>
		<title>默认开启tsdb,系统启动有异常</title>
		<body>使用的1.0.25版本,其中默认开启了tsdb 启动后会报错 ``` 2017-12-07 17:16:23.254 [destination = example , address = /127.0.0.1:3306 , EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /127.0.0.1:3306 has an error, retrying. caused by java.lang.NullPointerException: null         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:428) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:348) ~[canal.parse-1.0.25.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:164) ~[canal.parse-1.0.25.jar:na]         at java.lang.Thread.run(Unknown Source) [na:1.8.0_144] 2017-12-07 17:16:23.256 [destination = example , address = /127.0.0.1:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[java.lang.NullPointerException         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPositionInternal(MysqlEventParser.java:428)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.findStartPosition(MysqlEventParser.java:348)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:164)         at java.lang.Thread.run(Unknown Source) ``` 将 canal.instance.tsdb.spring.xml=classpath:spring/tsdb/h2-tsdb.xml 注释就好了 canal.instance.tsdb.enable设为false不起作用 另外希望有这些参数的详细说明</body>
		<created>2017-12-07 09:19:40</created>
		<closed>2017-12-27 09:34:25</closed>
	</bug>
	<bug>
		<id>439</id>
		<title>parse faield : CREATE TABLE `columns_priv`</title>
		<body>Example的日志发现这个错误</body>
		<created>2017-12-07 02:57:35</created>
		<closed>2017-12-27 09:41:23</closed>
	</bug>
	<bug>
		<id>360</id>
		<title>SocketChannelPool bug</title>
		<body>运行日志，请关注最后一行 ```` 2017-08-08 09:59:07.224 [destination = example , address = /127.0.0.1:3306 , EventParser] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv4Stack: false 2017-08-08 09:59:07.224 [destination = example , address = /127.0.0.1:3306 , EventParser] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv6Addresses: false 2017-08-08 09:59:07.313 [destination = example , address = /127.0.0.1:3306 , EventParser] DEBUG io.netty.util.NetUtil - Loopback interface: lo (Software Loopback Interface 1, 127.0.0.1) 2017-08-08 09:59:07.317 [destination = example , address = /127.0.0.1:3306 , EventParser] DEBUG io.netty.util.NetUtil - \proc\sys\net\core\somaxconn: 200 (non-existent) 2017-08-08 09:59:07.403 [destination = example , address = /127.0.0.1:3306 , EventParser] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.machineId: 00:00:00:00:00:00:00:e0 (auto-detected) 2017-08-08 09:59:07.404 [destination = example , address = /127.0.0.1:3306 , EventParser] DEBUG io.netty.util.internal.ThreadLocalRandom - -Dio.netty.initialSeedUniquifier: 0xfe50d10b389fdd4b 2017-08-08 09:59:07.482 [destination = example , address = /127.0.0.1:3306 , EventParser] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.allocator.type: pooled 2017-08-08 09:59:07.483 [destination = example , address = /127.0.0.1:3306 , EventParser] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.threadLocalDirectBufferSize: 65536 2017-08-08 09:59:07.483 [destination = example , address = /127.0.0.1:3306 , EventParser] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.maxThreadLocalCharBufferSize: 16384 2017-08-08 09:59:17.547 [destination = example , address = /127.0.0.1:3306 , EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect MysqlConnection to /127.0.0.1:3306... ````  ````java public static SocketChannel open(SocketAddress address) throws Exception {         final SocketChannel socket = new SocketChannel();         boot.connect(address).addListener(new ChannelFutureListener() {              @Override             public void operationComplete(ChannelFuture arg0) throws Exception {                 if (arg0.isSuccess()) socket.setChannel(arg0.channel());                 synchronized (socket) {                     socket.notify();    // &lt;--------------------                 }             }         });         synchronized (socket) {             socket.wait();  // &lt;--------------------         }         if (null == socket.getChannel()) {             throw new IOException("can't create socket!");         }         chManager.put(socket.getChannel(), socket);         return socket;     } ```` notify是有可能会比wait提前执行的，建议优化下      </body>
		<created>2017-08-08 02:07:05</created>
		<closed>2017-10-16 03:51:06</closed>
	</bug>
	<bug>
		<id>350</id>
		<title>canal解析json类型出错</title>
		<body>和#330中提到的问题一致，mysql版本是5.7 sql如下： a='"a"'+',"a"'*30000 sql="insert into canal_json_test(j_json) values('[%s]')" % (a) 即一个json字段的值为'["a","a","a"……]' 共30001个a，可以正常插入mysql，但是canal解析这个出错了</body>
		<created>2017-07-27 07:36:11</created>
		<closed>2017-08-03 13:15:47</closed>
	</bug>
	<bug>
		<id>330</id>
		<title>MySQL5.7 JSON解析问题</title>
		<body>canal 版本1.0.24 写入MySQL的JSON数据，Canal解析失败  2017-06-22 11:39:25.104 [destination = example , address = /127.0.0.1:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:example[com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: illegal json data         at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_array_or_object(JsonConversion.java:81)         at com.taobao.tddl.dbsync.binlog.JsonConversion.parse_value(JsonConversion.java:61)         at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:955)         at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:99)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:495)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:376)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:108)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:62)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:326)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:176)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:130)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745)   另外如果json中包含双引号，canal解析之后丢失了转义符号。 ![image](https://user-images.githubusercontent.com/5357638/27416733-e8f73ba8-5741-11e7-96c0-2a0de5ddc8af.png) </body>
		<created>2017-06-22 03:57:23</created>
		<closed>2017-08-03 13:15:38</closed>
	</bug>
	<bug>
		<id>275</id>
		<title>canal.instance.filter.regex 参数不起作用</title>
		<body>canal.instance.filter.regex = demodbname\\\\.user  这样设置后，client端还是会接受到接收到非user表的binlog日志信息。 使用版本： 1.0.23</body>
		<created>2017-03-06 06:35:40</created>
		<closed>2017-04-01 13:42:10</closed>
	</bug>
	<bug>
		<id>274</id>
		<title>解析binlog日志失败</title>
		<body>           canal运行后解析binlog日志的时候出现错误，提示Read Q_SQL_MODE_CODE error: limit excceed: 67 我看了canal的部分源码，貌似是sql_mode设置的不对？SQL_mode在canal里定义是64位的所以用getLong64，但是实际是67导致越界了？没看出数据库的数据有啥问题 PS: mysql版本是5.7.14   mysqlbinlog的日志： ------------------------------------------------------------------- 见附件 ![binlog](https://cloud.githubusercontent.com/assets/2059502/23546331/104ef6f0-003a-11e7-8f0c-40f50a85356c.png)  [binlog.txt](https://github.com/alibaba/canal/files/816403/binlog.txt)   canal实例日志： ----------------------------------------------------------------- 2017-03-03 17:17:19.952 [destination = gene_cachenotice_play3 , address = /172.16.10.213:3306 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position  {"identity":{"slaveId":-1,"sourceAddress":{"address":"172.16.10.213","port":3306}},"postion":{"included":false,"journalName":"mysql-bin.000016","position":24856433,"serverId":213,"timestamp":1488484784000}} 2017-03-03 17:17:19.959 [destination = gene_cachenotice_play3 , address = /172.16.10.213:3306 , EventParser] WARN  com.taobao.tddl.dbsync.binlog.LogDecoder - Decoding Query failed from: mysql-bin.000016:24858343 java.io.IOException: Read Q_SQL_MODE_CODE error: limit excceed: 67 at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.&lt;init&gt;(QueryLogEvent.java:477) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) ~[canal.parse.dbsync-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) [canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.23.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_25] Caused by: java.lang.IllegalArgumentException: limit excceed: 67 at com.taobao.tddl.dbsync.binlog.LogBuffer.getLong64(LogBuffer.java:873) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:566) ~[canal.parse.dbsync-1.0.23.jar:na] ... 6 common frames omitted 2017-03-03 17:17:19.960 [destination = gene_cachenotice_play3 , address = /172.16.10.213:3306 , EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address 172.16.10.213/172.16.10.213:3306 has an error, retrying. caused by  java.io.IOException: Read Q_SQL_MODE_CODE error: limit excceed: 67 at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.&lt;init&gt;(QueryLogEvent.java:477) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) ~[canal.parse.dbsync-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) ~[canal.parse-1.0.23.jar:na] at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) ~[canal.parse-1.0.23.jar:na] at java.lang.Thread.run(Thread.java:745) [na:1.8.0_25] Caused by: java.lang.IllegalArgumentException: limit excceed: 67 at com.taobao.tddl.dbsync.binlog.LogBuffer.getLong64(LogBuffer.java:873) ~[canal.parse.dbsync-1.0.23.jar:na] at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:566) ~[canal.parse.dbsync-1.0.23.jar:na] ... 6 common frames omitted 2017-03-03 17:17:19.960 [destination = gene_cachenotice_play3 , address = /172.16.10.213:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:gene_cachenotice_play3[java.io.IOException: Read Q_SQL_MODE_CODE error: limit excceed: 67 at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:650) at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.&lt;init&gt;(QueryLogEvent.java:477) at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:154) at com.taobao.tddl.dbsync.binlog.LogDecoder.decode(LogDecoder.java:106) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:123) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.IllegalArgumentException: limit excceed: 67 at com.taobao.tddl.dbsync.binlog.LogBuffer.getLong64(LogBuffer.java:873) at com.taobao.tddl.dbsync.binlog.event.QueryLogEvent.unpackVariables(QueryLogEvent.java:566) ... 6 more ]</body>
		<created>2017-03-03 09:39:48</created>
		<closed>2017-04-01 13:40:14</closed>
	</bug>
	<bug>
		<id>227</id>
		<title>canal仓库代码编译之后,不支持mysql5.1的问题</title>
		<body>hi canal开发团队,我使用canal仓库中master分支的代码编译. Mysql Server version: 5.1.73 错误信息: &gt; ERROR: [destination = example , address = test/192.168.10.16:3306 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlConnection - java.io.IOException: ErrorPacket [errorNumber=1193, fieldCount=-1, message=Unknown system variable 'binlog_checksum', sqlState=HY000, sqlStateMarker=#]  with command: set @master_binlog_checksum= @@global.binlog_checksum   changed file:https://github.com/alibaba/canal/commit/0c9eecd758a8cba0f218a121f83d3e8f01c4dee8  ![image](https://cloud.githubusercontent.com/assets/9983697/20620006/5bffa586-b332-11e6-80b6-3dd104c87c00.png)   这篇答案说`binlog_checksum`变量是从mysql5.6才开始支持.[参考](http://dba.stackexchange.com/questions/66226/mysql-slave-database-wont-start)  如果是这样的话,时候意味着canal以后不会支持mysql5.6以下版本?   thx   </body>
		<created>2016-11-25 09:17:38</created>
		<closed>2016-11-28 06:11:32</closed>
	</bug>
	<bug>
		<id>206</id>
		<title>SUBSCRIPTION存在bug</title>
		<body>直接上SessionHandler的代码段 `case SUBSCRIPTION:                     Sub sub = Sub.parseFrom(packet.getBody());                     if (StringUtils.isNotEmpty(sub.getDestination()) &amp;&amp; StringUtils.isNotEmpty(sub.getClientId())) {                         clientIdentity = new ClientIdentity(sub.getDestination(),                             Short.valueOf(sub.getClientId()),                             sub.getFilter());                         MDC.put("destination", clientIdentity.getDestination());                         embeddedServer.subscribe(clientIdentity);  ```                     // 尝试启动，如果已经启动，忽略                     if (!embeddedServer.isStart(clientIdentity.getDestination())) {                         ServerRunningMonitor runningMonitor = ServerRunningMonitors.getRunningMonitor(clientIdentity.getDestination());                         if (!runningMonitor.isStart()) {                             runningMonitor.start();                         }                     }                      ctx.setAttachment(clientIdentity);// 设置状态数据                     NettyUtils.ack(ctx.getChannel(), null);                 } else {                     NettyUtils.error(401,                         MessageFormatter.format("destination or clientId is null", sub.toString()).getMessage(),                         ctx.getChannel(),                         null);                 }                 break; ```  `  问题代码为：embeddedServer.subscribe(clientIdentity); instance还没有启动就可以执行subscribe？？？ 个人认为正确的逻辑应该是：先启动runningMonitor，再判断instance是否启动，最后再执行subscribe。  按目前的代码逻辑，会产生如下的问题： 1.两台canalserver和两台canalclient正在运行，其中有个instance名称为xxx 2.将xxx的Active置为false，xxx会被release掉，然后触发重新抢占 3.在抢占的过程中，zk上xxx/cluster节点和xxx/running节点的注册是有时间间隔的 4.因为有时间间隔，所以，按照ClusterNodeAccessStrategy的逻辑，canalclient在进行connect时拿到的ip不一定是以后抢占到running节点的那个canalserver的ip 5.即使ip有问题，仍然可以进行connect和subscribe操作，但在get的时候会报错，然后重连 6.虽然重连了，但是subscribe时产生了脏数据，position信息被缓存到metaManager了 7.过了几天xxx处于running状态的那台canal关了，此时slave-canal接管，会启动instance，但是metaManager已经启动过了，postion定位到了几天前 8.悲剧了  备注： 1.此处判断runningMonitor的目的是想触发lazy-start，但是lazy-start在HA模式下不成立，请参见 https://github.com/alibaba/canal/issues/205 2.强烈建议改造一下ClusterNodeAccessStrategy，HA模式下取Address的时候只认running节点，就甭支持lazy了，按现在的设计为了支持lazy徒增了复杂度，导致客户端的高可用切换的时间有时非常长 </body>
		<created>2016-09-28 02:57:53</created>
		<closed>2016-11-17 10:39:29</closed>
	</bug>
	<bug>
		<id>205</id>
		<title>instance的lazy启动在HA模式下不成立</title>
		<body>在HA模式下，instance的lazy启动不成立。  **首先看CanalController的代码** for (Map.Entry&lt;String, InstanceConfig&gt; entry : instanceConfigs.entrySet()) {             final String destination = entry.getKey();             InstanceConfig config = entry.getValue();             // 创建destination的工作节点             if (!config.getLazy() &amp;&amp; !embededCanalServer.isStart(destination)) {                 // HA机制启动                 ServerRunningMonitor runningMonitor = ServerRunningMonitors.getRunningMonitor(destination);                 if (!runningMonitor.isStart()) {                     runningMonitor.start();                 }             }  ```         if (autoScan) {             instanceConfigMonitors.get(config.getMode()).register(destination, defaultAction);         }     } ```  lazy模式下，ServerRunningMonitor不会启动，那么在zookeeper的/otter/canal/destinations/destination/cluster节点下不会有数据，running节点也不会有数据  **再看ClusterNodeAccessStrategy的代码**     public SocketAddress nextNode() {         if (runningAddress != null) {// 如果服务已经启动，直接选择当前正在工作的节点             return runningAddress;         } else if (!currentAddress.isEmpty()) { // 如果不存在已经启动的服务，可能服务是一种lazy启动，随机选择一台触发服务器进行启动             return currentAddress.get(0);// 默认返回第一个节点，之前已经做过shuffle         } else {             throw new CanalClientException("no alive canal server");         }     }  由于cluster和running节点都不存在，此处会进入else分支，客户端会一直报错，没办法触发laze启动。 </body>
		<created>2016-09-28 02:30:37</created>
		<closed>2016-11-17 10:39:13</closed>
	</bug>
	<bug>
		<id>201</id>
		<title>MysqlConnection的seek方法和dump方法会报NullPointerException</title>
		<body>seek方法和dump方法都有初始化LogContext  ``` java LogContext context = new LogContext(); ```  但是在decode的时候会报NullPointerException 建议分别添加  ``` java context.setLogPosition(new LogPosition(binlogfilename)); ``` </body>
		<created>2016-09-27 02:14:57</created>
		<closed>2016-09-29 02:31:22</closed>
	</bug>
	<bug>
		<id>171</id>
		<title>客户端高可用存在bug</title>
		<body>问题场景  1，canal有两台服务器c1和c2，c1处于激活状态 2，客户端使用ClusterCanalConnector进行消费，也是两台服务器d1和d2，d1处于激活状态 3，c1发生宕机，此时d1会执行【ClusterCanalConnector】的restart()方法。restart中有三步： 第一步，disconnect，会释放running节点； 第二步，线程休眠5秒钟； 第三步，尝试重连，重连的时候会执行initRunning()方法。 4，在d1释放running节点后，d2会被立即触发，执行initRunning()方法，initRunning中也有两大步： 第一步，抢占running节点 第二步，执行processActiveEnter()方法，该方法肯定会报错，因为此时d2连接的canal也是c1，执行initRunning的zk线程会异常退出，此时d2的mutex变量仍然为false，并且没有释放running节点 5，d1后续的重连都没意义了，因为d2没有释放running节点，所以d1的mutext变成了false 6，最终结果是d1和d2都阻塞了 7，此时关闭d2，d1的initRunning方法会被触发，但d1的processActiveEnter方法仍然会报错，d1仍然无法恢复消费 </body>
		<created>2016-04-25 12:31:55</created>
		<closed>2016-04-29 03:30:09</closed>
	</bug>
	<bug>
		<id>139</id>
		<title>关闭canal之后mysql连接未释放</title>
		<body>**使用canal时，发现当mysql库没有任何binlog产生时，关闭canal后部分mysql连接未释放。**  mysql版本：5.6.24-72.2-log canal版本：1.0.21  关闭前的堆栈信息如下：  ``` "destination = 90 , address = /192.168.12.110:3309 , HeartBeatTimeTask" daemon prio=10 tid=0x00007f4b2c00d000 nid=0x395b in Object.wait() [0x00007f4b51d13000]    java.lang.Thread.State: TIMED_WAITING (on object monitor)         at java.lang.Object.wait(Native Method)         at java.util.TimerThread.mainLoop(Timer.java:552)         - locked &lt;0x000000076052ffa8&gt; (a java.util.TaskQueue)         at java.util.TimerThread.run(Timer.java:505)  "destination = 90 , address = /192.168.12.110:3309 , EventParser" prio=10 tid=0x00007f4bd073e800 nid=0x395a runnable [0x00007f4b51d54000]    java.lang.Thread.State: RUNNABLE         at sun.nio.ch.FileDispatcherImpl.read0(Native Method)         at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)         at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)         at sun.nio.ch.IOUtil.read(IOUtil.java:197)         at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)         - locked &lt;0x0000000760a5efb0&gt; (a java.lang.Object)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209)         at java.lang.Thread.run(Thread.java:745) ```  关闭前的mysql连接信息如下：  ``` mysql&gt; show processlist; | 442033 | xxx | 192.168.201.174:47598 | NULL | Sleep       |       4 |                                                                       | NULL             |         1 |             1 | | 442035 | xxx | 192.168.201.174:47600 | NULL | Binlog Dump |       4 | Master has sent all binlog to slave; waiting for binlog to be updated | NULL             |         0 |             0 | ```  **执行./bin/stop.sh后，发现442035连接未释放！！！** ./log/90/90.log如下：  ``` 2016-01-14 11:55:11.655 [Thread-5] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - stop CannalInstance for 1-90 2016-01-14 11:55:11.670 [Thread-5] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.12.110:3309... 2016-01-14 11:55:11.675 [destination = 90 , address = /192.168.12.110:3309 , EventParser] INFO  c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - I/O interrupted while readi ng from client socket java.nio.channels.ClosedByInterruptException: null         at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) ~[na:1.7.0_55]         at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:412) ~[na:1.7.0_55]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch0(DirectLogFetcher.java:154) ~[canal.parse-1.0.21-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:70) ~[canal.parse-1.0.21-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:113) [canal.parse-1.0.21-SNAPSHOT.jar:na]         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:209) [canal.parse-1.0.21-SNAPSHOT.jar:na]         at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55] 2016-01-14 11:55:11.676 [destination = 90 , address = /192.168.12.110:3309 , EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /192.168.12.110 :3309 is not connected 2016-01-14 11:55:11.676 [destination = 90 , address = /192.168.12.110:3309 , EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.12.110:3309... 2016-01-14 11:55:11.677 [Thread-5] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - stop successful.... 2016-01-14 11:55:11.677 [Thread-5] INFO  c.a.otter.canal.server.embedded.CanalServerWithEmbedded - stop CanalInstances[90] successfully ```  另外，偶尔还会出现以下莫名mysql连接： ![image](https://cloud.githubusercontent.com/assets/7714843/12315487/4bcd3586-bab6-11e5-8a34-4c15758cab19.png) </body>
		<created>2016-01-14 04:00:29</created>
		<closed>2016-03-01 14:21:55</closed>
	</bug>
	<bug>
		<id>137</id>
		<title>java.lang.ArrayIndexOutOfBoundsException: 0 com.alibaba.otter.canal.parse.inbound.mysql.dbsync.SimpleDdlParser.parseTableName(SimpleDdlParser.java:163)</title>
		<body>版本1.0.19 错误如下： ![qq 20151218092517](https://cloud.githubusercontent.com/assets/11752250/11886867/b88a512a-a569-11e5-9dba-1feaa8a6637c.png)  binlog位置如下： ![qq 20151218092527](https://cloud.githubusercontent.com/assets/11752250/11886874/c864b91e-a569-11e5-8677-c808ef22c33d.png)   使用pt-online-schema-change变更字段后出现这种情况 </body>
		<created>2015-12-18 01:32:41</created>
		<closed>2015-12-30 05:56:20</closed>
	</bug>
	<bug>
		<id>135</id>
		<title>值由Null变为空字符串时，isUpdated属性为false</title>
		<body>mysql版本是5.5 将一个varchar类型的列的值由NULL，改为''时，从canal获取的entry的column里，isupdated属性为false。看了下LogEventConverter，感觉是isUpdated方法有个bug。newValue为""，旧值为NULL时，旧的column.getValue()为""。 </body>
		<created>2015-12-01 10:04:41</created>
		<closed>2015-12-30 05:57:49</closed>
	</bug>
	<bug>
		<id>130</id>
		<title>mysql 5.6版本 datetime值为null时 sqltype解析异常</title>
		<body>问题描述： mysql 版本为5.6.12， 在插入数据时，DATETIME类型的字段（如update_time字段）设置为NULL时，在从canal客户端获取到的数据中，对应字段的sqlType为1111（OTHER）  而在mysql 5.5.*版本中做同样的测试，DATETIME为null时，canal客户端获取到的对应sqlType为93（TIMESTAMP）  这个是BUG么？ </body>
		<created>2015-10-20 04:08:40</created>
		<closed>2015-11-13 06:17:38</closed>
	</bug>
	<bug>
		<id>112</id>
		<title>mysql5.6时间毫秒精度支持</title>
		<body>支持下mysql5.6毫秒精度的解析.  ## 测试case：  CREATE TABLE `t1` (   `id` int(11) NOT NULL AUTO_INCREMENT,   `time0` time DEFAULT NULL,   `time1` time(1) DEFAULT NULL,   `time2` time(2) DEFAULT NULL,   `time3` time(3) DEFAULT NULL,   `time4` time(4) DEFAULT NULL,   `time5` time(5) DEFAULT NULL,   `time6` time(6) DEFAULT NULL,   `timestamp0` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00',   `timestamp1` timestamp(1) NOT NULL DEFAULT '0000-00-00 00:00:00.0',   `timestamp2` timestamp(2) NOT NULL DEFAULT '0000-00-00 00:00:00.00',   `timestamp3` timestamp(3) NOT NULL DEFAULT '0000-00-00 00:00:00.000',   `timestamp4` timestamp(4) NOT NULL DEFAULT '0000-00-00 00:00:00.0000',   `timestamp5` timestamp(5) NOT NULL DEFAULT '0000-00-00 00:00:00.00000',   `timestamp6` timestamp(6) NOT NULL DEFAULT '0000-00-00 00:00:00.000000',   `datetime0` datetime DEFAULT NULL,   `datetime1` datetime(1) DEFAULT NULL,   `datetime2` datetime(2) DEFAULT NULL,   `datetime3` datetime(3) DEFAULT NULL,   `datetime4` datetime(4) DEFAULT NULL,   `datetime5` datetime(5) DEFAULT NULL,   `datetime6` datetime(6) DEFAULT NULL,   PRIMARY KEY (`id`) ## ) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8  测试数据: 1. insert into t1 values(null,'00:00:00.0','00:00:00.1','00:00:00.02','00:00:00.003','00:00:00.1004','00:00:00.1005','00:00:00.101016','2015-03-30 16:42:39.0','2015-03-30 16:42:39.0','2015-03-30 16:42:39.01','2015-03-30 16:42:39.032','2015-03-30 16:42:39.1023','2015-03-30 16:42:39.10132','2015-03-30 16:42:39.121232','2015-03-30 16:42:39.0','2015-03-30 16:42:39.0','2015-03-30 16:42:39.01','2015-03-30 16:42:39.032','2015-03-30 16:42:39.1023','2015-03-30 16:42:39.10132','2015-03-30 16:42:39.121232'); 2. insert into t1 values(null,'16:42:39.0','16:42:39.1','16:42:39.02','16:42:39.003','16:42:39.1004','16:42:39.1005','16:42:39.101016','2015-03-30 16:42:39.0','2015-03-30 16:42:39.0','2015-03-30 16:42:39.01','2015-03-30 16:42:39.032','2015-03-30 16:42:39.1023','2015-03-30 16:42:39.10132','2015-03-30 16:42:39.121232','2015-03-30 16:42:39.0','2015-03-30 16:42:39.0','2015-03-30 16:42:39.01','2015-03-30 16:42:39.032','2015-03-30 16:42:39.1023','2015-03-30 16:42:39.10132','2015-03-30 16:42:39.121232') 3. insert into t1 values(null,'-16:42:39.0','-16:42:39.1','-16:42:39.02','-16:42:39.003','16:42:39.1004','-16:42:39.1005','-16:42:39.101016','2015-03-30 16:42:39.0','2015-03-30 16:42:39.0','2015-03-30 16:42:39.01','2015-03-30 16:42:39.032','2015-03-30 16:42:39.1023','2015-03-30 16:42:39.10132','2015-03-30 16:42:39.121232','2015-03-30 16:42:39.0','2015-03-30 16:42:39.0','2015-03-30 16:42:39.01','2015-03-30 16:42:39.032','2015-03-30 16:42:39.1023','2015-03-30 16:42:39.10132','2015-03-30 16:42:39.121232'); </body>
		<created>2015-03-30 09:59:58</created>
		<closed>2015-04-10 04:39:58</closed>
	</bug>
	<bug>
		<id>105</id>
		<title>mysql5.6开启checksum后,基于时间查找位点会找到错误的位置</title>
		<body>问题描述：在mysql5.6之后版本中，因为引入了checksum机制，会在正常event的末尾加上4字节的checksum，而在做基于时间查找的seek方法中，decoder解析未识别FORMAT格式的event事件，导致没有判断出开启了checksum，所以导致正常的事务头的BEGIN，多了个4个字节，而判断是否为事务头采用了严格的字符串匹配，就因为多了4个字节，导致event类型出错。  影响范围：基于mysql5.6 + 开启了checksum + 涉及时间查找(比如主备切换和基于时间戳启动)  修改: 1.  增加format类型解析  ```  LogDecoder decoder = new LogDecoder();         decoder.handle(LogEvent.ROTATE_EVENT);         decoder.handle(LogEvent.FORMAT_DESCRIPTION_EVENT);         decoder.handle(LogEvent.QUERY_EVENT);         decoder.handle(LogEvent.XID_EVENT);         LogContext context = new LogContext();         while (fetcher.fetch()) {             LogEvent event = null;             event = decoder.decode(fetcher, context);              if (event == null) {                 throw new CanalParseException("parse failed");             }              if (!func.sink(event)) {                 break;             }         } ``` 1. 基于时间查找，增加事务begin/end的多重判断. </body>
		<created>2014-12-17 14:06:00</created>
		<closed>2014-12-17 14:12:46</closed>
	</bug>
	<bug>
		<id>96</id>
		<title>表黑名单默认关闭</title>
		<body>目前表黑名单定义针对spring模式，虽然配置了一个空值，针对AviaterRegexFilter空值意味着全匹配，所以所有的数据全部被当作黑名单过滤了  AviaterRegexFilter曾加个属性，特殊处理空值 </body>
		<created>2014-06-26 07:37:34</created>
		<closed>2014-06-26 07:44:57</closed>
	</bug>
	<bug>
		<id>87</id>
		<title>group模式开启了HeartBeat线程，出现数据阻塞</title>
		<body>otter issue : https://github.com/alibaba/otter/issues/46 </body>
		<created>2014-02-18 13:10:48</created>
		<closed>2014-02-18 13:13:48</closed>
	</bug>
	<bug>
		<id>86</id>
		<title>canal中针对group情况不支持非HeartBeat的HA方式</title>
		<body>支持下otter issue : https://github.com/alibaba/otter/issues/45 </body>
		<created>2014-02-18 13:09:03</created>
		<closed>2014-02-18 13:13:16</closed>
	</bug>
	<bug>
		<id>78</id>
		<title>default-instance.xml模式下，客户端订阅的filter条件在canal server切换后会丢失filter</title>
		<body> ZooKeeperMetaManager的listAllSubscribeInfo方法中只读取了clientId信息，并没有读取对应的filter，导致canal client提交的信息在canal server下一次启动时并没有载入，导致切换后客户端提交的filter并未生效.   ps . MemoryMetaManager/FileMetaManager模式下正常.   &lt;pre&gt;   String path = ZookeeperPathUtils.getDestinationPath(destination);         List&lt;String&gt; childs = null;         try {             childs = zkClientx.getChildren(path);         } catch (ZkNoNodeException e) {             // ignore         }          if (CollectionUtils.isEmpty(childs)) {             return new ArrayList&lt;ClientIdentity&gt;();         }         List&lt;Short&gt; clientIds = new ArrayList&lt;Short&gt;();         for (String child : childs) {             if (StringUtils.isNumeric(child)) {                 clientIds.add(ZookeeperPathUtils.getClientId(child));             }         }          Collections.sort(clientIds); // 进行一个排序         List&lt;ClientIdentity&gt; clientIdentities = Lists.newArrayList();         for (Short clientId : clientIds) {             clientIdentities.add(new ClientIdentity(destination, clientId));         }          return clientIdentities; } &lt;/pre&gt; </body>
		<created>2013-12-17 03:24:59</created>
		<closed>2013-12-17 03:26:15</closed>
	</bug>
	<bug>
		<id>76</id>
		<title>canal心跳检查出现ArrayIndexOutOfBoundsException</title>
		<body>&lt;pre&gt; Exception in thread "destination = transfer1 , address = /192.168.237.26:3306 , MysqlHeartBeatTimeTask" java.lang.ArrayIndexOutOfBoundsException: 1 at com.alibaba.otter.canal.parse.driver.mysql.utils.ByteHelper.readBinaryCodedLengthBytes(ByteHelper.java:83) at com.alibaba.otter.canal.parse.driver.mysql.packets.server.OKPacket.fromBytes(OKPacket.java:44) at com.alibaba.otter.canal.parse.driver.mysql.MysqlUpdateExecutor.update(MysqlUpdateExecutor.java:53) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.update(MysqlConnection.java:73) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser$MysqlHeartBeatTimeTask.run(MysqlEventParser.java:208) at java.util.TimerThread.mainLoop(Timer.java:512) at java.util.TimerThread.run(Timer.java:462) &lt;/pre&gt;   对应的心跳检查sql为; show master status; </body>
		<created>2013-12-03 02:35:11</created>
		<closed>2013-12-03 02:38:47</closed>
	</bug>
	<bug>
		<id>75</id>
		<title>heartbeat心跳检查程序出现Timer already cancelled.</title>
		<body>&lt;pre&gt; 2013-11-26 15:33:28.895 [destination = inter_ttsdb1 , address = /192.168.24.130:3306 , EventParser] WARN  c.a.o.canal.parse.inbound.mysql.dbsync.DirectLogFetcher - Received EOF packet from server, apparent master disconnected. 2013-11-26 15:33:28.895 [destination = inter_ttsdb1 , address = /192.168.24.130:3306 , EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.24.130:3306... 2013-11-26 15:33:28.896 [destination = inter_ttsdb1 , address = /192.168.24.130:3306 , EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - disConnect MysqlConnection to /192.168.24.130:3306... 2013-11-26 15:33:42.819 [destination = inter_ttsdb1 , address = /192.168.24.130:3306 , EventParser] INFO  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - start heart beat....  2013-11-26 15:33:42.825 [destination = inter_ttsdb1 , address = /192.168.24.130:3306 , EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /192.168.24.130:3306 has an error, retrying. caused by  java.lang.IllegalStateException: Timer already cancelled.     at java.util.Timer.sched(Timer.java:354) ~[na:1.6.0_20]     at java.util.Timer.schedule(Timer.java:222) ~[na:1.6.0_20]     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.startHeartbeat(MysqlEventParser.java:153) ~[canal.parse-1.0.7.jar:na]     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:83) ~[canal.parse-1.0.7.jar:na]     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:143) ~[canal.parse-1.0.7.jar:na]     at java.lang.Thread.run(Thread.java:619) [na:1.6.0_20] 2013-11-26 15:33:42.826 [destination = inter_ttsdb1 , address = /192.168.24.130:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:inter_ttsdb1[java.lang.IllegalStateException: Timer already cancelled.     at java.util.Timer.sched(Timer.java:354)     at java.util.Timer.schedule(Timer.java:222)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.startHeartbeat(MysqlEventParser.java:153)     at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:83)     at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:143)     at java.lang.Thread.run(Thread.java:619) ] 2013-11-26 15:33:42.826 [destination = inter_ttsdb1 , address = /192.168.24.130:3306 , EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /192.168.24.130:3306 is not connected 2013-11-26 15:33:42.826 [destination = inter_ttsdb1 , address = /192.168.24.130:3306 , EventParser] INFO  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - the channel /192.168.24.130:3306 is not connected &lt;/pre&gt;   异常问题分析： 1.  首先是出现mysql异常关闭，导致binlog dump和meta的mysql链接都出现断开 2.  MysqlHeartBeatTimeTask在下一次timer调度中，会出现异常，目前try...catch的异常只包含了SocketTimeoutException/IOException，如果出现其他，比如RuntimeException，就会导致Timer退出，并设置为cancel状态，触发本issue的bug.  </body>
		<created>2013-12-02 03:26:33</created>
		<closed>2013-12-02 03:31:04</closed>
	</bug>
	<bug>
		<id>74</id>
		<title>mysql create table语句解析错误</title>
		<body>&lt;pre&gt; public static final String TABLE_PATTERN        = "^(IF\\s*NOT\\s*EXIST\\s*)?(IF\\s*EXIST\\s*)?(`?.+?`?[;\\(\\s]+?)?.*$"; // 采用非贪婪模式 &lt;/pre&gt;   在mysql语法里正确为EXISTS </body>
		<created>2013-11-15 07:19:54</created>
		<closed>2013-11-15 07:29:15</closed>
	</bug>
	<bug>
		<id>70</id>
		<title>mysql bit类型支持</title>
		<body>测试的表：  &lt;pre&gt; CREATE TABLE `test_bit_all` (   id int(10) unsigned NOT NULL AUTO_INCREMENT,   bit1 bit(1),   bit2 bit(9),   bit3 bit(17),   bit4 bit(25),   bit5 bit(33),   bit6 bit(41),   bit7 bit(49),   bit8 bit(57),   bit9 bit(64),   PRIMARY KEY (`id`)   )  ENGINE=InnoDB DEFAULT CHARSET=gbk; &lt;/pre&gt;   构造数据： insert into test_bit_all(id,bit1,bit2,bit3,bit4,bit5,bit6,bit7,bit8,bit9) values(null,18446744073709551615,18446744073709551615,18446744073709551615,18446744073709551615,18446744073709551615,18446744073709551615,18446744073709551615,18446744073709551615,18446744073709551615) ;  输出结果： EventColumn[index=1,columnType=-7,columnName=bit1,columnValue=1,isNull=false,isKey=false,isUpdate=true] EventColumn[index=2,columnType=-7,columnName=bit2,columnValue=511,isNull=false,isKey=false,isUpdate=true] EventColumn[index=3,columnType=-7,columnName=bit3,columnValue=131071,isNull=false,isKey=false,isUpdate=true] EventColumn[index=4,columnType=-7,columnName=bit4,columnValue=33554431,isNull=false,isKey=false,isUpdate=true] EventColumn[index=5,columnType=-7,columnName=bit5,columnValue=8589934591,isNull=false,isKey=false,isUpdate=true] EventColumn[index=6,columnType=-7,columnName=bit6,columnValue=2199023255551,isNull=false,isKey=false,isUpdate=true] EventColumn[index=7,columnType=-7,columnName=bit7,columnValue=562949953421311,isNull=false,isKey=false,isUpdate=true] EventColumn[index=8,columnType=-7,columnName=bit8,columnValue=144115188075855871,isNull=false,isKey=false,isUpdate=true] EventColumn[index=9,columnType=-7,columnName=bit9,columnValue=18446744073709551615,isNull=false,isKey=false,isUpdate=true] </body>
		<created>2013-10-10 06:19:54</created>
		<closed>2013-10-10 06:20:58</closed>
	</bug>
	<bug>
		<id>69</id>
		<title>mysql登录失败</title>
		<body>&lt;pre&gt; Caused by: java.io.IOException: connect /192.168.1.206:3306 failure:java.lang.ArrayIndexOutOfBoundsException: 1         at com.alibaba.otter.canal.parse.driver.mysql.utils.ByteHelper.readUnsignedShortLittleEndian(ByteHelper.java:56)         at com.alibaba.otter.canal.parse.driver.mysql.packets.server.ErrorPacket.fromBytes(ErrorPacket.java:35)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:170)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:66)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:51)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:92)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:143)         at java.lang.Thread.run(Thread.java:662)          at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:69)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:51)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:92)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:143)         at java.lang.Thread.run(Thread.java:662) &lt;/pre&gt;   mysql version :  5.5.30-log </body>
		<created>2013-10-09 07:31:09</created>
		<closed>2013-10-11 06:34:57</closed>
	</bug>
	<bug>
		<id>66</id>
		<title>mysql binary类型处理</title>
		<body>otter issue : https://github.com/alibaba/otter/issues/22  针对同步binary类型，binlog里记录为String，导致后续在处理时按照字符串进行处理，出现错误.  </body>
		<created>2013-09-25 10:53:28</created>
		<closed>2013-09-25 13:38:38</closed>
	</bug>
	<bug>
		<id>65</id>
		<title>schema为纯数字时，desc 123.table会包语法错误，导致获取表结构异常</title>
		<body>&lt;pre&gt; mysql&gt; desc 123.pre_common_addon ; ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '123.pre_common_addon' at line 1 &lt;/pre&gt;   需要对schema / table name做一下转义符 </body>
		<created>2013-09-24 13:50:29</created>
		<closed>2013-09-24 14:18:33</closed>
	</bug>
	<bug>
		<id>62</id>
		<title>ddl语句解析异常</title>
		<body>针对如下ddl语句：  &lt;pre&gt; CREATE TABLE `cm_settle_incash` (    `batch_id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '批次',    `counter_acct_code` bigint(20) DEFAULT NULL COMMENT '柜台科目号',    `channel_sum_amount` decimal(18,2) NOT NULL COMMENT '总金额',    `channel_sum_cost` decimal(18,2) NOT NULL DEFAULT '0.00' COMMENT '总成本',    `bank_sum_amount` decimal(18,2) NOT NULL DEFAULT '0.00' COMMENT '总来款',    `begin_date` date DEFAULT NULL COMMENT '开始日期',    `end_date` date DEFAULT NULL COMMENT '结束日期',    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '更新时间',    `cdk_amount` decimal(18,2) DEFAULT '0.00' COMMENT '长短款金额',    `cdk_type` tinyint(4) DEFAULT NULL COMMENT '长短款类型 1长款2短款',    `deal_id` bigint(20) DEFAULT NULL COMMENT '交易id',    `cdk_deal_id` bigint(20) DEFAULT NULL COMMENT '长短款id',    PRIMARY KEY (`batch_id`)  ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8; &lt;/pre&gt;   通过SimpleDdlParser，无法解析正确的schema/table name，导致解析出错.   主要原因： 1.  SimpleDdlParser是通过正则进行表名提取，比如会处理`schema`.`table`的情况，如果对应整个ddl中有个.号，会导致schema的匹配出错 </body>
		<created>2013-09-12 04:04:43</created>
		<closed>2013-09-12 04:12:08</closed>
	</bug>
	<bug>
		<id>60</id>
		<title>SimpleDdlParser process query failed. </title>
		<body>出现类似异常：  &lt;pre&gt; pid:1 nid:1 exception:canal:6.21-&gt;6.20:com.alibaba.otter.canal.parse.exception.CanalParseException: com.alibaba.otter.canal.parse.exception.CanalParseException: SimpleDdlParser process query failed. pls submit issue with this queryString: CREATE table `bak591`.`j_order_log_back_201309` like j_order_log Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: SimpleDdlParser process query failed. pls submit issue with this queryString: CREATE table `bak591`.`j_order_log_back_201309` like j_order_log &lt;/pre&gt;   登录数据库，在bak库下，执行ddl语句，跨库建表，导致解析异常.  </body>
		<created>2013-09-03 10:20:34</created>
		<closed>2013-09-03 12:57:16</closed>
	</bug>
	<bug>
		<id>56</id>
		<title>canal server在开启heartbeat，遇到解析异常后会出现mysql链接暴涨</title>
		<body>canal server在开启heartbeat，遇到解析异常后会出现mysql链接暴涨.  用户在使用canal解析mysql 5.6.13版本时，解析时出现如下异常：  &lt;pre&gt; Caused by: com.alibaba.otter.canal.parse.exception.CanalParseException: parse row data failed. Caused by: java.lang.IllegalArgumentException: !! Unknown BLOB packlen = 0         at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.fetchValue(RowsLogBuffer.java:752)         at com.taobao.tddl.dbsync.binlog.event.RowsLogBuffer.nextValue(RowsLogBuffer.java:97)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseOneRow(LogEventConvert.java:294)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parseRowsEvent(LogEventConvert.java:249)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:102)         at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.LogEventConvert.parse(LogEventConvert.java:60)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser.parseAndProfilingIfNecessary(AbstractEventParser.java:297)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3$1.sink(AbstractEventParser.java:161)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.dump(MysqlConnection.java:120)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:189) &lt;/pre&gt;   运行一段时间后，mysql链接逐步持续增长，最终超过最大连接数 </body>
		<created>2013-08-22 07:58:48</created>
		<closed>2013-08-27 10:28:26</closed>
	</bug>
	<bug>
		<id>55</id>
		<title>client在server不可用时，出现cpu100%</title>
		<body>用户反馈：在linux上，ClusterCanalConnector这种方式里面只有一个地址的时候 如果这个宕机了 重连的速度非常快 cpu100%.    大致客户端代码： while(true) {     client.connect()     client.getWithoutAck()/client.ack(); }  windows下cpu不会100%，linux环境下明显.   </body>
		<created>2013-08-22 06:40:54</created>
		<closed>2013-08-22 06:52:37</closed>
	</bug>
	<bug>
		<id>50</id>
		<title>canal中输出表名全为小写</title>
		<body>canal 1.0.6版本之前，解析的schema , tablename 全部转化为了小写。  解决：需要保留下binlog中记录的内容，不做大小写转化  </body>
		<created>2013-08-06 08:29:10</created>
		<closed>2013-08-06 08:30:26</closed>
	</bug>
	<bug>
		<id>49</id>
		<title>canal server在windows下启动失败，conf目录下修改配置一直不生效</title>
		<body>canal deployer在打包的时候，将conf也打入到了jar中，而在一些特定的环境下，classloader会优先加载了jar包中的conf配置文件，从而导致在打包目录下的配置一直未生效.    也就是说：conf下和canal.deployer-xxxx.jar中都有canal.properties和instance.properties  解决办法： 1.  在canal.deployer-xxx.jar打包时去除conf目录.  </body>
		<created>2013-07-24 03:51:22</created>
		<closed>2013-07-24 03:53:58</closed>
	</bug>
	<bug>
		<id>48</id>
		<title>mysql主备切换采用虚ip切换，canal的failover机制</title>
		<body>采用虚ip进行mysql主备管理时，暴露的ip只有1个，目前canal识别主备切换是根据ip和对应meta信息中的ip地址发生不一致时，进行一次切换操作，基于时间戳重新定位binlog . 所以，当使用虚ip，后端发生了主备切换，前端无法感知，一直会出现对应binlog不存在的错误，一直重试，从而导致canal整个不可用  解决：在出现binlog不存在的错误时，尝试按照时间戳重新定位binlog .   什么情况下binlog会出现不存在： 1. 虚ip　mysql主备切换 2. canal解析延迟过久，对应binlog已经被删除.   针对情况2，需要避免。解决办法：当按照时间戳进行重新定位binlog时，如果当前所有binlog的时间戳都晚于查找的时间戳，那应该挂起，不能直接使用第一个binlog进行解析处理 </body>
		<created>2013-07-24 03:41:45</created>
		<closed>2013-08-22 03:24:41</closed>
	</bug>
	<bug>
		<id>47</id>
		<title>canal server启动时针对canal.ip配置为空时，默认监听为AddressUtils.getHostIp()的地址，导致多IP地址不可访问</title>
		<body>canal server启动时针对canal.ip配置为空时，默认监听为AddressUtils.getHostIp()的地址，导致多IP地址不可访问.     解决：  1.  如果配置ip地址为空时，默认socket的监听为 :${canal.port}，不指定ip绑定，接收任何该机器任何ip过来的请求.  2.  同时会通过AddressUtils.getHostIp()选择一个ip，暴露到zk中，由客户端进行访问.  </body>
		<created>2013-07-24 03:28:35</created>
		<closed>2013-07-24 03:33:19</closed>
	</bug>
	<bug>
		<id>43</id>
		<title>canal重启后会丢失client提交的filter条件，被instance.properties配置的filter给覆盖</title>
		<body>canal server在重启后，没有重新读取一次客户端subscribe()提交的filter记录，导致启动时重新载入了instance.properties，client filter条件被丢弃  解决：canal instance重新启动时，优先读取一次MetaManager中记录的client filter信息 </body>
		<created>2013-06-05 10:28:31</created>
		<closed>2013-06-05 11:04:36</closed>
	</bug>
	<bug>
		<id>39</id>
		<title>HA模式下mysql master-standby无法自动切换</title>
		<body>canal 主  canal config ## detecing config  canal.instance.detecting.enable = true canal.instance.detecting.sql = insert into retl.xdual values(1,now()) on duplicate key update x=now() canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = true #   example log   2013-05-14 10:23:29.509 [destination = example , address = /192.168.196.82:3306 , EventParser] WARN  c.alibaba.otter.canal.parse.driver.mysql.MysqlConnector - connect failed!java.net.ConnectException: Connection refused         at sun.nio.ch.Net.connect(Native Method)         at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:525)         at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:65)         at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:51)         at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:141)         at java.lang.Thread.run(Thread.java:679)  canal 从  ## detecing config  canal.instance.detecting.enable = true canal.instance.detecting.sql = insert into retl.xdual values(1,now()) on duplicate key update x=now() canal.instance.detecting.interval.time = 3 canal.instance.detecting.retry.threshold = 3 canal.instance.detecting.heartbeatHaEnable = true </body>
		<created>2013-05-14 04:32:54</created>
		<closed>2013-05-15 03:54:32</closed>
	</bug>
	<bug>
		<id>38</id>
		<title>scan定时扫描需要忽略canal meta/paser.dat文件</title>
		<body>默认配置中，canal选择的是file-intance.xml机制，会将相关数据定时刷新到文件. 而文件的存储默认是和instance的配置文件在一起.    比如 conf/example/meta.dat.   所以在scan定时扫描的时候，就会发现instance下的文件有所变化，导致需要进行一次reload.   解决： scan扫描时，忽略.dat的文件变更 </body>
		<created>2013-04-28 08:11:56</created>
		<closed>2013-04-28 08:46:35</closed>
	</bug>
	<bug>
		<id>37</id>
		<title>DDL语句不能及时输出</title>
		<body>ddl语句会在Transaction Buffer中被缓存中，只有在下一次出现Transaction Begin/End的消息时，才会输出到store中，client才可见  解决：ddl语句在binlog协议中，前后不会有begin/commit事件，需要特殊处理，进入Transaction Buffer后，也立马flush到store中  影响版本 &lt;= 1.0.3 </body>
		<created>2013-04-18 03:53:02</created>
		<closed>2013-04-18 03:53:58</closed>
	</bug>
	<bug>
		<id>32</id>
		<title>client基于zookeeper地址断开一次后，立马重新启动，偶儿会出现NPE异常</title>
		<body>问题描述： 1.  client基于zookeeper地址获取canal server的工作节点 2.  client第一次起来工作后，被非正常退出，比如kill.   然后立马又重新启动 3.  client会出现NullPointException异常  原因分析：  1.  client第一次启动后，会在zookeeper记录一个client running节点，非正常退出后，running节点不会立马小时 2.  当client下一次立马启动后，发现running节点存在，并不会立马创建socket，当执行后面的get数据操作时，发现socket为null，就出现了NPE问题  影响的版本 &lt;= 1.0.3.  </body>
		<created>2013-04-11 13:36:30</created>
		<closed>2013-04-11 14:08:36</closed>
	</bug>
	<bug>
		<id>29</id>
		<title>canal解析DDL操作出现异常，导致整个解析挂起</title>
		<body>问题描述:  1.  启动canal server/client 2.  执行ddl操作，比如create，alter , delete，create表的操作.  特定的case触发DDL表名解析错误 3.  canal在解析binlog后，发现binlog column信息和当前tablemeta cache中的数据不一致，抛出异常，进行重试.   存在的问题:  就是在第三步出现异常后，没有更新对应的cache数据，导致下一次解析时一直使用上一次错误的tablemeta，导致一直出错，一直在重试.  </body>
		<created>2013-04-10 05:43:20</created>
		<closed>2013-04-10 05:48:50</closed>
	</bug>
	<bug>
		<id>25</id>
		<title>windows bat脚本启动失败</title>
		<body>出错提示：   'conf_dir' 不是内部或外部命令，也不是可运行的程序 或批处理文件。 'canal_conf' 不是内部或外部命令，也不是可运行的程序 或批处理文件。 'logback_configurationFile' 不是内部或外部命令，也不是可运行的程序 或批处理文件。 Listening for transport dt_socket at address: 9099 Exception in thread "main" java.lang.NoClassDefFoundError: com/alibaba/otter/can al/deployer/CanalLauncher Caused by: java.lang.ClassNotFoundException: com.alibaba.otter.canal.deployer.Ca nalLauncher         at java.net.URLClassLoader$1.run(URLClassLoader.java:202)         at java.security.AccessController.doPrivileged(Native Method)         at java.net.URLClassLoader.findClass(URLClassLoader.java:190)         at java.lang.ClassLoader.loadClass(ClassLoader.java:307)         at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)         at java.lang.ClassLoader.loadClass(ClassLoader.java:248) Could not find the main class: com.alibaba.otter.canal.deployer.CanalLauncher. Program will exit. </body>
		<created>2013-03-26 07:16:47</created>
		<closed>2013-03-26 07:20:48</closed>
	</bug>
	<bug>
		<id>23</id>
		<title>canal client在第一次建立连接时，如果canal server所有都配置了lazy模式，client启动失败</title>
		<body>client启动时需要做一个check :  1.  如果当前instance已经有running的canal server，直接选择该节点，进行链接 2.  如果没有running节点，再从instance/cluster节点中，随机选择一个节点，进行链接 </body>
		<created>2013-03-19 11:24:58</created>
		<closed>2013-03-19 11:38:43</closed>
	</bug>
	<bug>
		<id>19</id>
		<title>mysql metaConnection链接泄漏</title>
		<body>运行一段时间后，与mysql之间的数据库链接达到了几千条，且全部都是处于ESTABLISHED状态。  进行jmap dump内存对象，却无法找到SocketChannelImpl的相关实例，说明是被full gc回收了.  </body>
		<created>2013-03-05 07:14:38</created>
		<closed>2013-03-05 07:34:35</closed>
	</bug>
	<bug>
		<id>18</id>
		<title>mysql text中文字符出现乱码</title>
		<body>mysql table中字段类型为text时，数据库编码和表编码均为utf-8，canal配置的解析编码为utf-8，解析出来的数据记录为乱码.    原因分析： 1. mysql binlog中将text/blob类型都记录为LogEvent.MYSQL_TYPE_BLOB 2. canal识别到BLOB信息，无法区分是text还是blob，都按照iso-8859-1进行编码，导致问题的产生  解决： 1.  拿到binlog后，针对BLOB类型，需要反查下table meta信息，获取真实的字段类型，区分出text，然后按照编码进行解析.  </body>
		<created>2013-02-27 10:20:28</created>
		<closed>2013-02-27 10:22:11</closed>
	</bug>
	<bug>
		<id>17</id>
		<title>mysql varchar类型处理'\000'字符问题</title>
		<body>线上测试遇到一个问题： a. 业务执行sql插入了一条记录，其中一个字段为：'210012\000\000\000' b. otter中美同步，更新了这条记录，将字段更新为： '210012'  (去除了\000) c. canal再一次解析时，发现before和after值相同，没有任何字段发生变更，导致otter同步sql执行失败。  原因分析： 1. dbsync在解析'210012\000\000\000'，等价于'210012'，自动忽略了'\000'请求  代码： for (; (found &lt; end) &amp;&amp; buf[found] != '\0'; found++)  说明： \0为c-style风格的字符串结束符，至于业务执行怎么插入了\000，目前暂未知原因 </body>
		<created>2013-02-26 14:15:32</created>
		<closed>2013-02-26 15:01:07</closed>
	</bug>
	<bug>
		<id>16</id>
		<title>column字段变更信息丢失</title>
		<body>在LogEventConvert中处理时column，信息不正确。  1. isUpdate所有都为true，正确的应该是：根据before和after字进行判断.  2. sqlType所有都为0，正确应该是：int类型对应于的java.sql.Types </body>
		<created>2013-02-26 14:05:44</created>
		<closed>2013-02-26 14:11:50</closed>
	</bug>
	<bug>
		<id>14</id>
		<title>cannal.properties里的配制选项是否轻微错误?</title>
		<body>在deployer工程里的canal.properties里的  canal.address=   是否应该改为  canal.ip=  因为在 CanalConstants 这个类里是如下定义的:  public static final String CANAL_IP                          = ROOT + "." + "ip"; </body>
		<created>2013-02-23 09:04:44</created>
		<closed>2013-02-24 06:46:48</closed>
	</bug>
	<bug>
		<id>11</id>
		<title>mysql driver在获取table meta时，针对表不存在出现阻塞</title>
		<body>mysql driver在处理返回的数据包没有考虑异常情况 </body>
		<created>2013-02-19 06:17:58</created>
		<closed>2013-02-19 06:18:16</closed>
	</bug>
	<bug>
		<id>7</id>
		<title>table meta是否主键信息判断错误</title>
		<body>Entry数据中isKey的数据返回错误，全部返回为true </body>
		<created>2013-02-19 04:17:52</created>
		<closed>2013-02-19 04:19:38</closed>
	</bug>
	<bug>
		<id>1</id>
		<title>修复memory-instance.xml下，不配置zkserver，可以正常启动和关闭</title>
		<body></body>
		<created>2013-02-06 02:30:35</created>
		<closed>2013-02-06 12:08:43</closed>
	</bug>
</bugs>
